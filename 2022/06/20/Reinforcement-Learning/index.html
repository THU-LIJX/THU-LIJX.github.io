<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"thu-lijx.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="有限马尔可夫决策过程（Markov Decision Process, MDP）一个MDP可以用一个四元组表示$M &#x3D; &lt; S, A, P, R&gt;$，其中  S: 一个有限的状态集合，$S &#x3D; {s_1, s_2, \cdots, s_n}$ A: 一个有限的动作集合，$A &#x3D; {a_1, a_2, \cdots, a_m}$ P: 状态转移概率，$P{ss’}^a &#x3D; Pr\left">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/index.html">
<meta property="og:site_name" content="Shin&#39;s notebook">
<meta property="og:description" content="有限马尔可夫决策过程（Markov Decision Process, MDP）一个MDP可以用一个四元组表示$M &#x3D; &lt; S, A, P, R&gt;$，其中  S: 一个有限的状态集合，$S &#x3D; {s_1, s_2, \cdots, s_n}$ A: 一个有限的动作集合，$A &#x3D; {a_1, a_2, \cdots, a_m}$ P: 状态转移概率，$P{ss’}^a &#x3D; Pr\left">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/TD(0).png">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/sarsa.png">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/q-learning.png">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/Policy-Gradient.png">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/Policy-Gradient-w-baseline.png">
<meta property="og:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/Actor-Critic.png">
<meta property="article:published_time" content="2022-06-20T09:29:09.000Z">
<meta property="article:modified_time" content="2022-07-17T13:42:21.000Z">
<meta property="article:author" content="Shin">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/TD(0).png">

<link rel="canonical" href="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>Reinforcement Learning | Shin's notebook</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Shin's notebook</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>Tags</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>Categories</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://thu-lijx.github.io/2022/06/20/Reinforcement-Learning/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Shin">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Shin's notebook">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Reinforcement Learning
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-06-20 17:29:09" itemprop="dateCreated datePublished" datetime="2022-06-20T17:29:09+08:00">2022-06-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-17 21:42:21" itemprop="dateModified" datetime="2022-07-17T21:42:21+08:00">2022-07-17</time>
              </span>

          <br>
            <span class="post-meta-item" title="Symbols count in article">
              <span class="post-meta-item-icon">
                <i class="far fa-file-word"></i>
              </span>
                <span class="post-meta-item-text">Symbols count in article: </span>
              <span>7k</span>
            </span>
            <span class="post-meta-item" title="Reading time">
              <span class="post-meta-item-icon">
                <i class="far fa-clock"></i>
              </span>
                <span class="post-meta-item-text">Reading time &asymp;</span>
              <span>6 mins.</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="有限马尔可夫决策过程（Markov-Decision-Process-MDP）"><a href="#有限马尔可夫决策过程（Markov-Decision-Process-MDP）" class="headerlink" title="有限马尔可夫决策过程（Markov Decision Process, MDP）"></a>有限马尔可夫决策过程（Markov Decision Process, MDP）</h3><p>一个MDP可以用一个四元组表示$M = &lt; S, A, P, R&gt;$，其中</p>
<ul>
<li><em>S</em>: 一个有限的状态集合，$S = {s_1, s_2, \cdots, s_n}$</li>
<li><em>A</em>: 一个有限的动作集合，$A = {a_1, a_2, \cdots, a_m}$</li>
<li><em>P</em>: 状态转移概率，$P<em>{ss’}^a = Pr\left[S</em>{t+1} = s’ | S_t = s, A_t = a\right]$</li>
<li><em>R</em>: Reward函数，$R<em>s^a = E\left[R</em>{t+1} | S_t = s, A_t = a\right]$</li>
</ul>
<p>定义Return为</p>
<script type="math/tex; mode=display">
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}</script><p>性质：$G<em>t = R</em>{t+1} + \gamma G_{t+1}$，其中$0 \leq \gamma \leq 1$, 当$\gamma=1$时，需要确保每条序列都终止。<br><span id="more"></span></p>
<h4 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h4><p>令策略为$\pi$。</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \mathbb{E_\pi}\left[G_t|S_t = s\right] \\
    &q_{\pi}(s, a) = \mathbb{E_\pi}\left[G_t \ | \ S_t=s, A_t = a \right]
\end{align*}</script><h4 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_\pi(s')) \\
    &q_{\pi}(s,a) = R_s^a + \gamma\sum_{s'\in S} P_{ss'}^a \sum_{a' \in A} \pi(a'|s')q_\pi(s', a')
\end{align*}</script><p>写成期望形式</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_\pi(s) = \mathbb{E_\pi}[R_{t+1} + \gamma v_\pi(S_{t+1})| S_t = s] \\
    &q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})|S_t = s, A_t = a]
\end{align*}</script><p>写成矩阵形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n)
\end{bmatrix} = 
\begin{bmatrix}
    \mathcal{R}_{s_1} \\
    \vdots \\
    \mathcal{R}_{s_n}
\end{bmatrix} + \gamma
\begin{bmatrix}
    P_{s_1s_1} & \cdots & P_{s_1s_n} \\
    \vdots & & \vdots \\
    P_{s_ns_1} & \cdots & P_{s_ns_n} \\
\end{bmatrix}
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n) 
\end{bmatrix}</script><p>其中$\mathcal{R}<em>s = \mathbb{E}[R</em>{t+1} | S<em>t = s]$, $P</em>{ss’} = Pr[S_{t+1}=s’ | S_t = s]$。求解复杂度为$O(n^3)$，通常不会使用。</p>
<h4 id="Optimal-Policies-and-Optimal-Value-Function"><a href="#Optimal-Policies-and-Optimal-Value-Function" class="headerlink" title="Optimal Policies and Optimal Value Function"></a>Optimal Policies and Optimal Value Function</h4><p>$\pi \geq \pi’$当且仅当对于任意s，都满足$v<em>\pi(s) &gt; v</em>{\pi’}(s)$</p>
<p>记最优策略为$\pi_{*}$，则有</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi_*}(s) = \max_{\pi} v_{\pi}(s) \\
    &q_{\pi_*}(s, a) = \max_{\pi} q_{\pi}(s, a)
\end{align*}</script><h4 id="Bellman-Optimal-Equation"><a href="#Bellman-Optimal-Equation" class="headerlink" title="Bellman Optimal Equation"></a>Bellman Optimal Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_*(s) = \max_{a} q_{\pi_*}(s, a) \\
    &q_*(s, a) = R_s^a + \gamma\sum_{s' \in S} P_{ss'}^a v_*(s')
\end{align*}</script><h3 id="使用动态规划方法求解MDP"><a href="#使用动态规划方法求解MDP" class="headerlink" title="使用动态规划方法求解MDP"></a>使用动态规划方法求解MDP</h3><h4 id="策略评估（Policy-Evaluation）"><a href="#策略评估（Policy-Evaluation）" class="headerlink" title="策略评估（Policy Evaluation）"></a>策略评估（Policy Evaluation）</h4><p>策略评估指的是给定一个策略$\pi$，求出其价值函数$v_\pi$的过程。根据Bellman Equation，可以使用如下更新公式</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_k(s'))</script><p>具体计算时，既可以采用两个数组分别记录$v<em>{k+1}, v_k$，也可以只用一个数组就地更新，两种方式都可以收敛到$v</em>\pi$, 但收敛速度会有所不同。</p>
<h4 id="策略提升（Policy-Improvement）"><a href="#策略提升（Policy-Improvement）" class="headerlink" title="策略提升（Policy Improvement）"></a>策略提升（Policy Improvement）</h4><p>策略提升指的是根据给定策略$\pi$，得到一个新的策略$\pi’$，使得$\pi’ \geq \pi$。</p>
<p>使用如下方式对策略进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
    \pi'(s) &:= \text{argmax}_{a} q_\pi(s, a) \\
            &= \text{argmax}_{a} R_s^a + \gamma \sum_{s' \in S}P_{ss'}^av_\pi(s')
\end{align*}</script><p>上述方法是有效的，证明如下，假设$\pi$和$\pi’$都是确定性策略，且对于任意$s\in S$，都有</p>
<script type="math/tex; mode=display">
    q_\pi(s, \pi'(s)) \geq v_\pi(s)</script><p>则</p>
<script type="math/tex; mode=display">
\begin{align*}
    v_\pi(s) &\leq q_\pi(s, \pi'(s)) \\
             &\leq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=t, A_t=\pi'(s)] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma\mathbb{E}[R_{t+2} + \gamma v_\pi(S_{t+2})| S_{t+1}, A_{t+1}=\pi'(S_{t+1})] | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(S_{t+2}) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t=s, A_t=\pi'(s)] \\
             &= v_{\pi'}(s)
\end{align*}</script><h4 id="策略迭代（Policy-Iteration）"><a href="#策略迭代（Policy-Iteration）" class="headerlink" title="策略迭代（Policy Iteration）"></a>策略迭代（Policy Iteration）</h4><ol>
<li>策略评估</li>
<li>策略提升</li>
<li>重复1、2直到收敛</li>
</ol>
<h4 id="价值迭代（Value-Iteration）"><a href="#价值迭代（Value-Iteration）" class="headerlink" title="价值迭代（Value Iteration）"></a>价值迭代（Value Iteration）</h4><p>使用Bellman Optimal Equation直接计算出$v_{*}(s)$, 迭代公式为</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) := \max_a R_s^a + \gamma \sum_{s' \in S} P_{ss'}^av_k(s)</script><p>令$\pi_{*}(s)$为</p>
<script type="math/tex; mode=display">
   \pi_{*}(s) = \text{argmax}_a q_*(s, a)</script><h3 id="Temporal-Difference-Learning-时序差分学习"><a href="#Temporal-Difference-Learning-时序差分学习" class="headerlink" title="Temporal Difference Learning (时序差分学习)"></a>Temporal Difference Learning (时序差分学习)</h3><p>与蒙特卡洛方法一致，时序差分方法也可以直接从与环境互动的经验中学习策略，而不需要构建关于环境动态特性的模型。与动态规划相一致的是，时序差分方法也可以基于已得到的其他状态的估计值来更新当前状态的价值函数。在时序差分学习算法中，n-步算法将TD方法和MC方法联系在一起、TD（$\lambda$）能无缝地统一TD方法和MC方法。</p>
<p>MC方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[G_t - V(S_t)\right]</script><p>TD方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]</script><p>其中$\delta<em>t = R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t)$也称作TD error。</p>
<h4 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a>TD(0)</h4><p>更新公式：$V(S<em>t) \gets V(S_t) + \alpha \left[R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/TD(0).png" class="">
<h4 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma Q(S<em>{t+1}, A</em>{t+1}) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/sarsa.png" class="">
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma \max<em>{a’}Q(S</em>{t+1}, a’) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/q-learning.png" class="">
<h3 id="函数近似"><a href="#函数近似" class="headerlink" title="函数近似"></a>函数近似</h3><p>对价值函数进行参数化建模，用参数为$\theta$的函数$\hat{v}(s, \pmb{w})$去近似$v<em>\pi(s)$。指定一个状态的分布$\mu(s) \geq 0, \sum</em>{s} \mu(s) = 1$来表示我们对于每个状态s的误差的重视程度。一个自然的目标函数为均方价值误差，记为$VE$</p>
<script type="math/tex; mode=display">
VE(\pmb{w}) = \sum_{s\in S}\mu(s)\left[v_\pi(s) - \hat{v}(s, \pmb{w})\right]^2</script><p>假设在样本中的分布$\mu$和VE中的分布相同。在这种情况下，一个好的策略就是尽量减少观测到的样本的误差。使用SGD方法进行优化</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{w}_{t+1} &= \pmb{w}_t - \frac{1}{2}\alpha\nabla\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]^2 \\
&= \pmb{w}_t - \alpha\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]\nabla\hat{v}(S_t, \pmb{w}_t)
\end{align*}</script><p>由于我们无法得到$v<em>\pi(S_t)$，所以使用它的一个随机近似$U_t$替代，如果$U_t$是一个无偏估计，那么收敛性是有保证的。可以取$U_t$为$G_t$，这种方法被称做梯度方法。如果采用自举估计值，取$U_t$为$R</em>{t+1}+\hat{v}(S_{t+1}, \pmb{w})$，则没有足够的收敛性保证，但在大多数情况下，都会收敛，这种方法称为半梯度方法。</p>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>基于动作价值函数的方法，都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，策略的获得依赖于价值函数。策略梯度方法对策略进行参数化建模，通过梯度上升的方式直接对策略进行学习。用$\pmb{\theta}$表示策略的参数向量。</p>
<p>策略参数的学习方法都基于某种性能度量$J(\pmb{\theta})$的梯度，性能度量可以是从初始状态s到终止状态s’的return的期望。在能够分出episode和连续型的条件下，性能度量会有所不同。在得到梯度后，通过梯度上升，不断对策略的参数进行优化</p>
<script type="math/tex; mode=display">
\pmb{\theta_{t+1}} = \pmb{\theta_{t}} + \alpha \widehat{\nabla J(\pmb{\theta_{t}})}</script><p>其中，$\widehat{\nabla J(\pmb{\theta<em>{t}})}$是对$\nabla J(\pmb{\theta</em>{t}})$的随机估计。</p>
<h4 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h4><p>在分幕式情况下策略梯度定理表达式如下</p>
<script type="math/tex; mode=display">
\nabla J(\pmb{\theta}) \propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta})</script><p>其中，$\mu(s)$表示在使用策略$\pi$的条件下，s出现在轨迹中的概率。</p>
<h4 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h4><p>由于$\mu(s)$是将目标策略$\pi$下每个状态出现的频率作为加权系数的求和项，如果按照策略$\pi$执行，则状态将按这个比例出现。假设$\gamma$为1即没有折扣，根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \mathbb{E}_\pi\left[\sum_{a} q_\pi(S_t, a)\nabla \pi (a|S_t, \pmb{\theta})\right] \\
&= \mathbb{E}_\pi\left[\sum_{a} \pi(a|S_t, \pmb{\theta})q_\pi(S_t, a)\frac{\nabla \pi (a|S_t, \pmb{\theta})}{\pi(a|S_t, \pmb{\theta})}\right] \\
&= \mathbb{E}_\pi\left[q_\pi(S_t, A_t)\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{用采样$A_t \sim \pi$替换$a$} \\
&= \mathbb{E}_\pi\left[G_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{$\mathbb{E}_\pi\left[G_t|S_t, A_t\right] = q_\pi(S_t, A_t)$} \\
&= \mathbb{E}_\pi\left[G_t\nabla \ln \pi (A_t|S_t, \pmb{\theta})\right] \tag{$\nabla\ln x = \frac{\nabla x}{x}$}
\end{align*}</script><p>则可以得到REINFORCE算法<br><img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient.png" class=""></p>
<h4 id="REINFORCE-with-baseline"><a href="#REINFORCE-with-baseline" class="headerlink" title="REINFORCE with baseline"></a>REINFORCE with baseline</h4><p>根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \sum_s \mu(s) \sum_{a} (q_\pi(s, a)-b(s))\nabla \pi (a|s, \pmb{\theta})
\end{align*}</script><p>由此导出更新公式</p>
<script type="math/tex; mode=display">
\pmb{\theta}_{t+1} = \pmb{\theta}_{t} + \alpha\left(G_t - b(S_t)\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}</script><p>其中的$b(s)$可以是不随动作$a$变化的任意函数。因为这个baseline也可以是常量0，所以上式是REINFORCE算法更一般的推广。通常来说，加入baseline不会使更新的期望值发生变化，但是对方差会有很大影响。</p>
<img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient-w-baseline.png" class="">
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h4><p>尽管带基线的强化学习方法既学习了一个策略函数也学习了一个状态价值函数，我们也不认为它是一种AC方法，因为它的状态价值函数仅被用作baseline，而不是作为一个Critic，也即没有被用于自举操作。只有当采用自举法时，才会出现依赖于函数逼近质量的偏差和渐进性收敛。引入自举，往往能减小方差和加速收敛。</p>
<p>考虑单步AC方法，使用单步回报代替$G_t$，则得到如下更新公式</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{\theta}_{t+1} &= \pmb{\theta}_{t} + \alpha\left(G_{t:t+1} - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\left(R_{t+1} + \hat{v}(S_{t+1}, \pmb{w}) - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\delta_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}
\end{align*}</script><p>进而可以得到如下算法</p>
<img src="/2022/06/20/Reinforcement-Learning/Actor-Critic.png" class="">
<h3 id="Deep-Reinforcement-Learning-DRL"><a href="#Deep-Reinforcement-Learning-DRL" class="headerlink" title="Deep Reinforcement Learning (DRL)"></a>Deep Reinforcement Learning (DRL)</h3><h4 id="Deep-Q-Learning-DQN"><a href="#Deep-Q-Learning-DQN" class="headerlink" title="Deep Q-Learning (DQN)"></a>Deep Q-Learning (DQN)</h4><h4 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient (DDPG)"></a>Deep Deterministic Policy Gradient (DDPG)</h4><h4 id="Trust-Region-Policy-Gradient-TRPO"><a href="#Trust-Region-Policy-Gradient-TRPO" class="headerlink" title="Trust Region Policy Gradient (TRPO)"></a>Trust Region Policy Gradient (TRPO)</h4><h4 id="Proximal-Policy-Gradient-PPO"><a href="#Proximal-Policy-Gradient-PPO" class="headerlink" title="Proximal Policy Gradient (PPO)"></a>Proximal Policy Gradient (PPO)</h4><h4 id="Soft-Actor-Critic-SAC"><a href="#Soft-Actor-Critic-SAC" class="headerlink" title="Soft Actor-Critic (SAC)"></a>Soft Actor-Critic (SAC)</h4><p>参考文献：</p>
<ol>
<li><a target="_blank" rel="noopener" href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">《Reinforcement Learning: An Introduction》</a></li>
</ol>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item"></div>
      <div class="post-nav-item">
    <a href="/2022/07/10/Variational-Inference/" rel="next" title="Variational Inference">
      Variational Inference <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E9%99%90%E9%A9%AC%E5%B0%94%E5%8F%AF%E5%A4%AB%E5%86%B3%E7%AD%96%E8%BF%87%E7%A8%8B%EF%BC%88Markov-Decision-Process-MDP%EF%BC%89"><span class="nav-number">1.</span> <span class="nav-text">有限马尔可夫决策过程（Markov Decision Process, MDP）</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Value-Function"><span class="nav-number">1.1.</span> <span class="nav-text">Value Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bellman-Equation"><span class="nav-number">1.2.</span> <span class="nav-text">Bellman Equation</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Optimal-Policies-and-Optimal-Value-Function"><span class="nav-number">1.3.</span> <span class="nav-text">Optimal Policies and Optimal Value Function</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Bellman-Optimal-Equation"><span class="nav-number">1.4.</span> <span class="nav-text">Bellman Optimal Equation</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92%E6%96%B9%E6%B3%95%E6%B1%82%E8%A7%A3MDP"><span class="nav-number">2.</span> <span class="nav-text">使用动态规划方法求解MDP</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%AF%84%E4%BC%B0%EF%BC%88Policy-Evaluation%EF%BC%89"><span class="nav-number">2.1.</span> <span class="nav-text">策略评估（Policy Evaluation）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%8F%90%E5%8D%87%EF%BC%88Policy-Improvement%EF%BC%89"><span class="nav-number">2.2.</span> <span class="nav-text">策略提升（Policy Improvement）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E8%BF%AD%E4%BB%A3%EF%BC%88Policy-Iteration%EF%BC%89"><span class="nav-number">2.3.</span> <span class="nav-text">策略迭代（Policy Iteration）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%B7%E5%80%BC%E8%BF%AD%E4%BB%A3%EF%BC%88Value-Iteration%EF%BC%89"><span class="nav-number">2.4.</span> <span class="nav-text">价值迭代（Value Iteration）</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Temporal-Difference-Learning-%E6%97%B6%E5%BA%8F%E5%B7%AE%E5%88%86%E5%AD%A6%E4%B9%A0"><span class="nav-number">3.</span> <span class="nav-text">Temporal Difference Learning (时序差分学习)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#TD-0"><span class="nav-number">3.1.</span> <span class="nav-text">TD(0)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Sarsa"><span class="nav-number">3.2.</span> <span class="nav-text">Sarsa</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Q-Learning"><span class="nav-number">3.3.</span> <span class="nav-text">Q-Learning</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%87%BD%E6%95%B0%E8%BF%91%E4%BC%BC"><span class="nav-number">4.</span> <span class="nav-text">函数近似</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Policy-Gradient"><span class="nav-number">5.</span> <span class="nav-text">Policy Gradient</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E5%AE%9A%E7%90%86"><span class="nav-number">5.1.</span> <span class="nav-text">策略梯度定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#REINFORCE"><span class="nav-number">5.2.</span> <span class="nav-text">REINFORCE</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#REINFORCE-with-baseline"><span class="nav-number">5.3.</span> <span class="nav-text">REINFORCE with baseline</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Actor-Critic"><span class="nav-number">5.4.</span> <span class="nav-text">Actor-Critic</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Deep-Reinforcement-Learning-DRL"><span class="nav-number">6.</span> <span class="nav-text">Deep Reinforcement Learning (DRL)</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Q-Learning-DQN"><span class="nav-number">6.1.</span> <span class="nav-text">Deep Q-Learning (DQN)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Deep-Deterministic-Policy-Gradient-DDPG"><span class="nav-number">6.2.</span> <span class="nav-text">Deep Deterministic Policy Gradient (DDPG)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Trust-Region-Policy-Gradient-TRPO"><span class="nav-number">6.3.</span> <span class="nav-text">Trust Region Policy Gradient (TRPO)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Proximal-Policy-Gradient-PPO"><span class="nav-number">6.4.</span> <span class="nav-text">Proximal Policy Gradient (PPO)</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Soft-Actor-Critic-SAC"><span class="nav-number">6.5.</span> <span class="nav-text">Soft Actor-Critic (SAC)</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Shin</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/thu-lijx" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;thu-lijx" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:@gmail.com" title="E-Mail → mailto:@gmail.com" rel="noopener" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Shin</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-area"></i>
    </span>
    <span title="Symbols count total">21k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="Reading time total">20 mins.</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
