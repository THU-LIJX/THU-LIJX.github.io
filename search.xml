<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Reinforcement Learning</title>
    <url>/2022/06/20/Reinforcement-Learning/</url>
    <content><![CDATA[<h3 id="有限马尔可夫决策过程（Markov-Decision-Process-MDP）"><a href="#有限马尔可夫决策过程（Markov-Decision-Process-MDP）" class="headerlink" title="有限马尔可夫决策过程（Markov Decision Process, MDP）"></a>有限马尔可夫决策过程（Markov Decision Process, MDP）</h3><p>一个MDP可以用一个四元组表示$M = &lt; S, A, P, R&gt;$，其中</p>
<ul>
<li><em>S</em>: 一个有限的状态集合，$S = {s_1, s_2, \cdots, s_n}$</li>
<li><em>A</em>: 一个有限的动作集合，$A = {a_1, a_2, \cdots, a_m}$</li>
<li><em>P</em>: 状态转移概率，$P<em>{ss’}^a = Pr\left[S</em>{t+1} = s’ | S_t = s, A_t = a\right]$</li>
<li><em>R</em>: Reward函数，$R<em>s^a = E\left[R</em>{t+1} | S_t = s, A_t = a\right]$</li>
</ul>
<p>定义Return为</p>
<script type="math/tex; mode=display">
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}</script><p>性质：$G<em>t = R</em>{t+1} + \gamma G_{t+1}$，其中$0 \leq \gamma \leq 1$, 当$\gamma=1$时，需要确保每条序列都终止。<br><span id="more"></span></p>
<h4 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h4><p>令策略为$\pi$。</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \mathbb{E_\pi}\left[G_t|S_t = s\right] \\
    &q_{\pi}(s, a) = \mathbb{E_\pi}\left[G_t \ | \ S_t=s, A_t = a \right]
\end{align*}</script><h4 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_\pi(s')) \\
    &q_{\pi}(s,a) = R_s^a + \gamma\sum_{s'\in S} P_{ss'}^a \sum_{a' \in A} \pi(a'|s')q_\pi(s', a')
\end{align*}</script><p>写成期望形式</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_\pi(s) = \mathbb{E_\pi}[R_{t+1} + \gamma v_\pi(S_{t+1})| S_t = s] \\
    &q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})|S_t = s, A_t = a]
\end{align*}</script><p>写成矩阵形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n)
\end{bmatrix} = 
\begin{bmatrix}
    \mathcal{R}_{s_1} \\
    \vdots \\
    \mathcal{R}_{s_n}
\end{bmatrix} + \gamma
\begin{bmatrix}
    P_{s_1s_1} & \cdots & P_{s_1s_n} \\
    \vdots & & \vdots \\
    P_{s_ns_1} & \cdots & P_{s_ns_n} \\
\end{bmatrix}
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n) 
\end{bmatrix}</script><p>其中$\mathcal{R}<em>s = \mathbb{E}[R</em>{t+1} | S<em>t = s]$, $P</em>{ss’} = Pr[S_{t+1}=s’ | S_t = s]$。求解复杂度为$O(n^3)$，通常不会使用。</p>
<h4 id="Optimal-Policies-and-Optimal-Value-Function"><a href="#Optimal-Policies-and-Optimal-Value-Function" class="headerlink" title="Optimal Policies and Optimal Value Function"></a>Optimal Policies and Optimal Value Function</h4><p>$\pi \geq \pi’$当且仅当对于任意s，都满足$v<em>\pi(s) &gt; v</em>{\pi’}(s)$</p>
<p>记最优策略为$\pi_{*}$，则有</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi_*}(s) = \max_{\pi} v_{\pi}(s) \\
    &q_{\pi_*}(s, a) = \max_{\pi} q_{\pi}(s, a)
\end{align*}</script><h4 id="Bellman-Optimal-Equation"><a href="#Bellman-Optimal-Equation" class="headerlink" title="Bellman Optimal Equation"></a>Bellman Optimal Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_*(s) = \max_{a} q_{\pi_*}(s, a) \\
    &q_*(s, a) = R_s^a + \gamma\sum_{s' \in S} P_{ss'}^a v_*(s')
\end{align*}</script><h3 id="使用动态规划方法求解MDP"><a href="#使用动态规划方法求解MDP" class="headerlink" title="使用动态规划方法求解MDP"></a>使用动态规划方法求解MDP</h3><h4 id="策略评估（Policy-Evaluation）"><a href="#策略评估（Policy-Evaluation）" class="headerlink" title="策略评估（Policy Evaluation）"></a>策略评估（Policy Evaluation）</h4><p>策略评估指的是给定一个策略$\pi$，求出其价值函数$v_\pi$的过程。根据Bellman Equation，可以使用如下更新公式</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_k(s'))</script><p>具体计算时，既可以采用两个数组分别记录$v<em>{k+1}, v_k$，也可以只用一个数组就地更新，两种方式都可以收敛到$v</em>\pi$, 但收敛速度会有所不同。</p>
<h4 id="策略提升（Policy-Improvement）"><a href="#策略提升（Policy-Improvement）" class="headerlink" title="策略提升（Policy Improvement）"></a>策略提升（Policy Improvement）</h4><p>策略提升指的是根据给定策略$\pi$，得到一个新的策略$\pi’$，使得$\pi’ \geq \pi$。</p>
<p>使用如下方式对策略进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
    \pi'(s) &:= \text{argmax}_{a} q_\pi(s, a) \\
            &= \text{argmax}_{a} R_s^a + \gamma \sum_{s' \in S}P_{ss'}^av_\pi(s')
\end{align*}</script><p>上述方法是有效的，证明如下，假设$\pi$和$\pi’$都是确定性策略，且对于任意$s\in S$，都有</p>
<script type="math/tex; mode=display">
    q_\pi(s, \pi'(s)) \geq v_\pi(s)</script><p>则</p>
<script type="math/tex; mode=display">
\begin{align*}
    v_\pi(s) &\leq q_\pi(s, \pi'(s)) \\
             &\leq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=t, A_t=\pi'(s)] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma\mathbb{E}[R_{t+2} + \gamma v_\pi(S_{t+2})| S_{t+1}, A_{t+1}=\pi'(S_{t+1})] | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(S_{t+2}) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t=s, A_t=\pi'(s)] \\
             &= v_{\pi'}(s)
\end{align*}</script><h4 id="策略迭代（Policy-Iteration）"><a href="#策略迭代（Policy-Iteration）" class="headerlink" title="策略迭代（Policy Iteration）"></a>策略迭代（Policy Iteration）</h4><ol>
<li>策略评估</li>
<li>策略提升</li>
<li>重复1、2直到收敛</li>
</ol>
<h4 id="价值迭代（Value-Iteration）"><a href="#价值迭代（Value-Iteration）" class="headerlink" title="价值迭代（Value Iteration）"></a>价值迭代（Value Iteration）</h4><p>使用Bellman Optimal Equation直接计算出$v_{*}(s)$, 迭代公式为</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) := \max_a R_s^a + \gamma \sum_{s' \in S} P_{ss'}^av_k(s)</script><p>令$\pi_{*}(s)$为</p>
<script type="math/tex; mode=display">
   \pi_{*}(s) = \text{argmax}_a q_*(s, a)</script><h3 id="Temporal-Difference-Learning-时序差分学习"><a href="#Temporal-Difference-Learning-时序差分学习" class="headerlink" title="Temporal Difference Learning (时序差分学习)"></a>Temporal Difference Learning (时序差分学习)</h3><p>与蒙特卡洛方法一致，时序差分方法也可以直接从与环境互动的经验中学习策略，而不需要构建关于环境动态特性的模型。与动态规划相一致的是，时序差分方法也可以基于已得到的其他状态的估计值来更新当前状态的价值函数。在时序差分学习算法中，n-步算法将TD方法和MC方法联系在一起、TD（$\lambda$）能无缝地统一TD方法和MC方法。</p>
<p>MC方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[G_t - V(S_t)\right]</script><p>TD方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]</script><p>其中$\delta<em>t = R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t)$也称作TD error。</p>
<h4 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a>TD(0)</h4><p>更新公式：$V(S<em>t) \gets V(S_t) + \alpha \left[R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/TD(0).png" class="">
<h4 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma Q(S<em>{t+1}, A</em>{t+1}) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/sarsa.png" class="">
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma \max<em>{a’}Q(S</em>{t+1}, a’) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/q-learning.png" class="">
<h3 id="函数近似"><a href="#函数近似" class="headerlink" title="函数近似"></a>函数近似</h3><p>对价值函数进行参数化建模，用参数为$\theta$的函数$\hat{v}(s, \pmb{w})$去近似$v<em>\pi(s)$。指定一个状态的分布$\mu(s) \geq 0, \sum</em>{s} \mu(s) = 1$来表示我们对于每个状态s的误差的重视程度。一个自然的目标函数为均方价值误差，记为$VE$</p>
<script type="math/tex; mode=display">
VE(\pmb{w}) = \sum_{s\in S}\mu(s)\left[v_\pi(s) - \hat{v}(s, \pmb{w})\right]^2</script><p>假设在样本中的分布$\mu$和VE中的分布相同。在这种情况下，一个好的策略就是尽量减少观测到的样本的误差。使用SGD方法进行优化</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{w}_{t+1} &= \pmb{w}_t - \frac{1}{2}\alpha\nabla\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]^2 \\
&= \pmb{w}_t - \alpha\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]\nabla\hat{v}(S_t, \pmb{w}_t)
\end{align*}</script><p>由于我们无法得到$v<em>\pi(S_t)$，所以使用它的一个随机近似$U_t$替代，如果$U_t$是一个无偏估计，那么收敛性是有保证的。可以取$U_t$为$G_t$，这种方法被称做梯度方法。如果采用自举估计值，取$U_t$为$R</em>{t+1}+\hat{v}(S_{t+1}, \pmb{w})$，则没有足够的收敛性保证，但在大多数情况下，都会收敛，这种方法称为半梯度方法。</p>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>基于动作价值函数的方法，都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，策略的获得依赖于价值函数。策略梯度方法对策略进行参数化建模，通过梯度上升的方式直接对策略进行学习。用$\pmb{\theta}$表示策略的参数向量。</p>
<p>策略参数的学习方法都基于某种性能度量$J(\pmb{\theta})$的梯度，性能度量可以是从初始状态s到终止状态s’的return的期望。在能够分出episode和连续型的条件下，性能度量会有所不同。在得到梯度后，通过梯度上升，不断对策略的参数进行优化</p>
<script type="math/tex; mode=display">
\pmb{\theta_{t+1}} = \pmb{\theta_{t}} + \alpha \widehat{\nabla J(\pmb{\theta_{t}})}</script><p>其中，$\widehat{\nabla J(\pmb{\theta<em>{t}})}$是对$\nabla J(\pmb{\theta</em>{t}})$的随机估计。</p>
<h4 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h4><p>在分幕式情况下策略梯度定理表达式如下</p>
<script type="math/tex; mode=display">
\nabla J(\pmb{\theta}) \propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta})</script><p>其中，$\mu(s)$表示在使用策略$\pi$的条件下，s出现在轨迹中的概率。</p>
<h4 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h4><p>由于$\mu(s)$是将目标策略$\pi$下每个状态出现的频率作为加权系数的求和项，如果按照策略$\pi$执行，则状态将按这个比例出现。假设$\gamma$为1即没有折扣，根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \mathbb{E}_\pi\left[\sum_{a} q_\pi(S_t, a)\nabla \pi (a|S_t, \pmb{\theta})\right] \\
&= \mathbb{E}_\pi\left[\sum_{a} \pi(a|S_t, \pmb{\theta})q_\pi(S_t, a)\frac{\nabla \pi (a|S_t, \pmb{\theta})}{\pi(a|S_t, \pmb{\theta})}\right] \\
&= \mathbb{E}_\pi\left[q_\pi(S_t, A_t)\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{用采样$A_t \sim \pi$替换$a$} \\
&= \mathbb{E}_\pi\left[G_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{$\mathbb{E}_\pi\left[G_t|S_t, A_t\right] = q_\pi(S_t, A_t)$} \\
&= \mathbb{E}_\pi\left[G_t\nabla \ln \pi (A_t|S_t, \pmb{\theta})\right] \tag{$\nabla\ln x = \frac{\nabla x}{x}$}
\end{align*}</script><p>则可以得到REINFORCE算法<br><img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient.png" class=""></p>
<h4 id="REINFORCE-with-baseline"><a href="#REINFORCE-with-baseline" class="headerlink" title="REINFORCE with baseline"></a>REINFORCE with baseline</h4><p>根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \sum_s \mu(s) \sum_{a} (q_\pi(s, a)-b(s))\nabla \pi (a|s, \pmb{\theta})
\end{align*}</script><p>由此导出更新公式</p>
<script type="math/tex; mode=display">
\pmb{\theta}_{t+1} = \pmb{\theta}_{t} + \alpha\left(G_t - b(S_t)\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}</script><p>其中的$b(s)$可以是不随动作$a$变化的任意函数。因为这个baseline也可以是常量0，所以上式是REINFORCE算法更一般的推广。通常来说，加入baseline不会使更新的期望值发生变化，但是对方差会有很大影响。</p>
<img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient-w-baseline.png" class="">
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h4><p>尽管带基线的强化学习方法既学习了一个策略函数也学习了一个状态价值函数，我们也不认为它是一种AC方法，因为它的状态价值函数仅被用作baseline，而不是作为一个Critic，也即没有被用于自举操作。只有当采用自举法时，才会出现依赖于函数逼近质量的偏差和渐进性收敛。引入自举，往往能减小方差和加速收敛。</p>
<p>考虑单步AC方法，使用单步回报代替$G_t$，则得到如下更新公式</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{\theta}_{t+1} &= \pmb{\theta}_{t} + \alpha\left(G_{t:t+1} - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\left(R_{t+1} + \hat{v}(S_{t+1}, \pmb{w}) - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\delta_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}
\end{align*}</script><p>进而可以得到如下算法</p>
<img src="/2022/06/20/Reinforcement-Learning/Actor-Critic.png" class="">
<h3 id="Deep-Reinforcement-Learning-DRL"><a href="#Deep-Reinforcement-Learning-DRL" class="headerlink" title="Deep Reinforcement Learning (DRL)"></a>Deep Reinforcement Learning (DRL)</h3><h4 id="Deep-Q-Learning-DQN"><a href="#Deep-Q-Learning-DQN" class="headerlink" title="Deep Q-Learning (DQN)"></a>Deep Q-Learning (DQN)</h4><h4 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient (DDPG)"></a>Deep Deterministic Policy Gradient (DDPG)</h4><h4 id="Trust-Region-Policy-Gradient-TRPO"><a href="#Trust-Region-Policy-Gradient-TRPO" class="headerlink" title="Trust Region Policy Gradient (TRPO)"></a>Trust Region Policy Gradient (TRPO)</h4><h4 id="Proximal-Policy-Gradient-PPO"><a href="#Proximal-Policy-Gradient-PPO" class="headerlink" title="Proximal Policy Gradient (PPO)"></a>Proximal Policy Gradient (PPO)</h4><h4 id="Soft-Actor-Critic-SAC"><a href="#Soft-Actor-Critic-SAC" class="headerlink" title="Soft Actor-Critic (SAC)"></a>Soft Actor-Critic (SAC)</h4><p>参考文献：</p>
<ol>
<li><a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">《Reinforcement Learning: An Introduction》</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Variational Inference</title>
    <url>/2022/07/10/Variational-Inference/</url>
    <content><![CDATA[<p>假设随机变量$X$和$Z$服从联合分布$p(x, z; \theta)$，其中$\theta$是分布的参数。我们可以得到$X$的观测值，但不能得到$Z$的观测值，即$X$为观测变量，$Z$为隐变量。给定$\theta$，并且有观测值$x$，我们希望得到$Z$的后验分布$p(z; x, \theta)$。通常这个后验分布无法直接计算，但可以通过一个变分分布$q(z)$近似。’<br><span id="more"></span></p>
<h3 id="Kullback-Leibler-divergence-KL散度"><a href="#Kullback-Leibler-divergence-KL散度" class="headerlink" title="Kullback-Leibler divergence (KL散度)"></a>Kullback-Leibler divergence (KL散度)</h3><p>对于离散分布$P$和$Q$，KL散度定义为</p>
<script type="math/tex; mode=display">
D_{KL}\left(P \ ||\ Q\right) = \sum_{x \in \mathcal{X}} P(x)\log\left(\frac{P(x)}{Q(x)}\right)</script><p>对于连续分布$P$和$Q$，KL散度定义为</p>
<script type="math/tex; mode=display">
D_{KL}\left(P \ ||\ Q\right) = \int_{-\infty}^{+\infty} P(x)\log\left(\frac{P(x)}{Q(x)}\right) dx</script><h3 id="Evidence-Lower-Bound-ELBO"><a href="#Evidence-Lower-Bound-ELBO" class="headerlink" title="Evidence Lower Bound (ELBO)"></a>Evidence Lower Bound (ELBO)</h3><p>evidence定义为$\log p(x; \theta)$。假设随机变量$Z$服从分布$q$，则可以推出evidence的一个下界。</p>
<script type="math/tex; mode=display">
\begin{align*}
\log p(x; \theta) &= \log \int p(x, z; \theta) dz \\
&= \log \int q(z) \frac{p(x, z; \theta)}{q(z)} dz \\
&= \log \mathbb{E}_{Z\sim q}\left[\frac{p(x, Z)}{q(Z)}\right] \\
&\geq \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z)}{q(Z)} \right] \tag{Jensen's Inequality} \\
\end{align*}</script><p><strong>Jenson’ s Inequality: </strong>$\mathbb{E}[\varphi(X)] \geq \varphi(\mathbb{E}[X])$，其中$\varphi$是一个凸函数。</p>
<p>将$\mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z)}{q(Z)}\right]$ 定义为<strong>ELBO</strong>，显然，ELBO和evidence之间存在一个gap，而这个gap变分分布$q(z)$和真实分布$p(z;x, \theta)$的KL散度，推导如下。</p>
<script type="math/tex; mode=display">
\begin{align*}
KL(q(z) \ || \ p(z|x, \theta)) &:= \mathbb{E}_{Z\sim q}\left[\log \frac{q(Z)}{p(Z; x, \theta)} \right] \\
&= \mathbb{E}_{Z\sim q}\left[\log q(Z) \right] - \mathbb{E}_{Z\sim q}\left[\log\frac{p(x, Z; \theta)}{p(x; \theta)} \right] \\
&= \mathbb{E}_{Z\sim q}\left[\log q(Z) \right] - \mathbb{E}_{Z\sim q}\left[\log p(x, Z; \theta) \right] + \mathbb{E}_{Z\sim q}\left[\log p(x; \theta) \right] \\
&= \log p(x; \theta) - \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z; \theta)}{q(Z)} \right] \\
&= evidence - ELBO
\end{align*}</script><h4 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h4><p>在得到观测数据$x$之后，我们希望做极大似然估计求$\theta$，但由于边缘分布$p(x; \theta) = \frac{p(x,z; \theta)}{p(z;x, \theta)}$无法直接计算，所以希望使用一个变分分布$q(z)$去近似$p(z; x, \theta)$，从而能够计算$p(x; \theta)$。为了使$q(z)$较好地近似$p(z; x, \theta)$，可以通过减小它们之间的KL散度实现，也即最大化ELBO。</p>
<h4 id="Expectation-Maximization-EM算法"><a href="#Expectation-Maximization-EM算法" class="headerlink" title="Expectation-Maximization (EM算法)"></a>Expectation-Maximization (EM算法)</h4><ul>
<li><strong>Expectation Step: </strong>$q^* = \arg\max<em>{q} ELBO =  \arg\max</em>{q} \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z; \theta)}{q(Z)} \right]$</li>
<li><strong>Maximization Step: </strong>$\theta<em>{new} = \arg \max</em>{\theta} ELBO = \arg\max<em>{\theta}\mathbb{E}</em>{Z\sim {q^\star}}\left[\log \frac{p(x, Z; \theta)}{q^\star(Z)} \right]$</li>
<li>重复这个过程直到收敛</li>
</ul>
<p>E步相当于在给定$\theta$的情况下，找到一个最优的$q(z)$，去逼近$p(z;x, \theta)$，使得通过变分得到估计尽量准确；M步相当于，在得到了一个较为准确的估计的条件下，做极大似然估计。</p>
<p>参考文章：<a href="https://mbernste.github.io/posts/elbo/">ELBO</a></p>
]]></content>
  </entry>
</search>
