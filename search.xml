<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Course Computer Graphics</title>
    <url>/2022/10/02/Course-Computer-Graphics/</url>
    <content><![CDATA[<h2 id="计算机图形学课程笔记"><a href="#计算机图形学课程笔记" class="headerlink" title="计算机图形学课程笔记"></a>计算机图形学课程笔记</h2><h3 id="Lecture-3-图形的扫描转换与区域填充"><a href="#Lecture-3-图形的扫描转换与区域填充" class="headerlink" title="Lecture 3 图形的扫描转换与区域填充"></a>Lecture 3 图形的扫描转换与区域填充</h3><p><strong>光栅图形学</strong></p>
<ul>
<li>基本图形的扫描转换 (scan conversion) ，如直线、圆弧、椭圆弧</li>
<li>多边形的扫描转换与区域填充 (area filling)</li>
<li>裁剪 (clipping)</li>
<li>反走样 (antialiasing)</li>
<li>投影 (projection)</li>
<li>消隐 (visible-surface detection / hidden-surface elimintaion)</li>
</ul>
<h4 id="直线的扫描转换"><a href="#直线的扫描转换" class="headerlink" title="直线的扫描转换"></a>直线的扫描转换</h4><p>常用算法：</p>
<ul>
<li>数值微分法 (DDA)</li>
<li>中点画线法</li>
<li><strong>Bresenham算法</strong></li>
</ul>
<p><strong>数值微分法</strong></p>
<ul>
<li>基本思想<ul>
<li>知道了两个端点$P_0(x_0, y_0), P_1(x_1, y_1)$，则可以求出直线段的斜率$k$。当$0 &lt; k &lt; 1$时，根据公式$y = kx + b$来计算相应$y$坐标，取像素点$(x, \text{round}(y))$作为当前点的坐标；当$k \geq 1$时，根据$y$求取$x$的坐标</li>
</ul>
</li>
<li>增量算法<ul>
<li>在一个迭代算法中，每一步的$x, y$值使用上一步的值加上一个增量来获得</li>
<li>$y<em>{i+1} = y_i + k \Delta x$，当$\Delta x = 1$时，$y</em>{i+1} = y_i + k$</li>
</ul>
</li>
<li>优缺点：方法直观但效率低</li>
</ul>
<p><strong>中点画线法</strong></p>
<ul>
<li>基本思想<ul>
<li>设当前像素点为$(x_p, y_p)$，下一个像素点为右侧$P_1$或右上角$P_2$两个点中的一个，设$M = (x_p + 1, y_p + 0.5)$为$P_1, P_2$的中点，$Q$为理想直线与$x = x_p + 1$的交点。将Q与M的y坐标值进行比较<ul>
<li>当$M$在$Q$点下方时，则右上角的$P_2$为下一个像素点</li>
<li>当$M$在$Q$点上方时，则右侧的$P_1$为下一个像素点</li>
</ul>
</li>
<li>构造判别式，也即将$M$的坐标带入直线方程，$d = F(M) = a(x_p + 1) + b(y_p + 0.5) + c$<ul>
<li>$a = y_0 - y_1, b = x_1 - x_0, c = x_0y_1 - x_1y_0$</li>
<li>$d &gt; 0$，则$M$在Q点上方，取$P_1$</li>
<li>$d &lt; 0$，则M在Q点下方，取$P_2$</li>
<li>$d = 0$，M、Q重合，此时取$P_1$或$P_2$都行</li>
</ul>
</li>
</ul>
</li>
<li>增量算法<ul>
<li>若当前像素处于$d \geq 0$的情况，则取$P_1$，要判断下一个像素位置，应计算，$d_1 = F(x_p+2, y_p +0.5) = a(x_p+2) + b(y_p+0.5) + c = d + a$</li>
<li>若当前像素处于$d &lt; 0$的情况，则取$P_2$，要判断下一个像素位置，应计算，$d_2 = F(x_p+2, y_p + 1.5) = a(x_p + 2) + b(y_p + 1.5) + c = d + a + b$</li>
</ul>
</li>
</ul>
<p><strong>Bresenham算法</strong></p>
<ul>
<li>基本思想<ul>
<li>设直线方程为$y<em>{i+1} = y_i + k(x</em>{i+1} - x_i) = y_i + k$，因为直线的起始点在像素中心，所以误差项$d$的初始值为0</li>
<li>当$d \geq 0.5$时，取$P_2$</li>
<li>当$d &lt; 0.5$时，取$P_1$</li>
<li>$x$的下标每增加1，$d$的值就相应递增直线的斜率$k$，当$d \geq 0.5$时，将$d$减去1</li>
</ul>
</li>
</ul>
<h4 id="圆弧的扫描转换"><a href="#圆弧的扫描转换" class="headerlink" title="圆弧的扫描转换"></a>圆弧的扫描转换</h4><p>算法：</p>
<ul>
<li>中点画圆法</li>
<li>正负法</li>
<li>多边形逼近法</li>
</ul>
<p><strong>中点画圆法</strong></p>
<ul>
<li>基本思想<ul>
<li>由于圆的对称性以及坐标轴的对称性，只需要画出八分之一圆弧即可得到整个圆弧</li>
<li>考虑中心在原点，半径为$R$的圆在第一象限内$x\in [0, R/\sqrt{2}]$的八分之一圆弧</li>
<li>构造判别式$F(x, y) = x^2 + y^2 - R^2$</li>
<li>已知$P(x_p, y_p), P_1(x_p+1,y_p), P_2(x_p+1, y_p-1), M(x_p+1, y_p-0.5)$</li>
<li>$d = F(M) = F(x_p+1, y_p-0.5) = (x_p+1)^2 + (y_p-0.5)^2 - R^2$<ul>
<li>若$d &lt; 0$，则取$P_1$为下一像素，而且再下一像素的判别式为$d’ = F(x_p+2, y_p-0.5) = d+2x_p+3$</li>
<li>若$d \geq 0$，则取$P_2$为下一像素，而且再下一像素的判别式为$d’ = F(x_p+2, y_p-1.5) = d+2x_p-2y_p+5$</li>
<li>第一个像素是$(0, R)$，判别式$d$的初始值是$d_0=F(1, R-0.5) = 1.25-R$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>正负法</strong></p>
<ul>
<li>基本思想<ul>
<li>由圆方程的隐函数$F(x, y) = x^2 + y^2 - R^2$，判断点与曲线的关系</li>
<li>已知$P<em>i(x_i, y_i)$，求$P</em>{i+1}$的原则<ul>
<li>当$F(x<em>i, y_i) \leq 0$，则$P</em>{i+1} = (x<em>i+1, y_i), F(x</em>{i+1}, y_{i+1}) = F(x_i, y_i) +2x_i +1$</li>
<li>当$F(x<em>i, y_i) \leq 0$，则$P</em>{i+1} = (x<em>i, y_i+1), F(x</em>{i+1}, y_{i+1}) = F(x_i, y_i) +2y_i +1$</li>
</ul>
</li>
</ul>
</li>
</ul>
<h4 id="椭圆的扫描转换"><a href="#椭圆的扫描转换" class="headerlink" title="椭圆的扫描转换"></a>椭圆的扫描转换</h4><p><strong>中点画法</strong></p>
<ul>
<li>基本思想<ul>
<li>由椭圆的对称性，可以只考虑第一象限椭圆弧生成</li>
<li>将椭圆弧分为上下两部分，以椭圆弧上切线斜率为-1处作为分界点</li>
<li>判别式$F(x, y) = b^2x^2 + a^2y^2 - a^2b^2$</li>
</ul>
</li>
</ul>
<h4 id="多边形的扫描转换"><a href="#多边形的扫描转换" class="headerlink" title="多边形的扫描转换"></a>多边形的扫描转换</h4><ul>
<li>多边形有两种重要的表示方式：<strong>顶点表示</strong>和<strong>点阵表示</strong></li>
<li>多边形的扫描转换：把多边形的顶点表示转换为点阵表示</li>
</ul>
<p>算法：</p>
<ul>
<li>逐点判断法<ul>
<li>点在多边形内的判别方法<ul>
<li>射线法</li>
<li>累计角度法</li>
</ul>
</li>
</ul>
</li>
<li>扫描线算法</li>
<li>边缘填充法</li>
<li><strong>边界标志法</strong></li>
</ul>
<p><strong>逐点判断法</strong></p>
<ul>
<li>射线法<ul>
<li>从点<em>V</em>发出射线，与多边形相交，若交点个数为偶数，则<em>V</em>点在多边形外边，反之在多边形内</li>
<li>若交点为多边形顶点，则需要判断该顶点对应的两条边是否在上下同侧</li>
</ul>
</li>
<li>累计角度法<ul>
<li>从$V$点向多边形$P$的各顶点发出射线，形成有向角，计算有向角之和<ul>
<li>和为0，则在外部</li>
<li>和为$\pm 2\pi$，则在内部</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>扫描线算法</strong></p>
<ul>
<li>基本思想<ul>
<li>按扫描线顺序，计算扫描线与多边形的相交区间，再用要求的颜色显示这些区间的像素，完成填充</li>
<li>一条扫描线填充过程的步骤<ul>
<li>求交</li>
<li>排序</li>
<li>配对</li>
<li>着色</li>
</ul>
</li>
</ul>
</li>
<li>细节<ul>
<li>扫描线与多边形的顶点相交时，必须正确进行交点的取舍</li>
<li>检查共享该顶点的两条边的另外两个端点的$y$值。交点个数为这两个点$y$值大于交点$y$值的个数</li>
<li>问题：求交、排序导致算法效率低</li>
<li>数据结构<ul>
<li><strong>活性边表 (AET, Active Edge Table)</strong><ul>
<li>把与当前扫描线相交的边称为活性边，将它们按照与扫描线交点$x$坐标升序排列</li>
<li>链表节点内容<ul>
<li>$x:$ 当前扫描线与边的交点的$x$坐标</li>
<li>$\Delta x$：从当前扫描线到下一条扫描线间的$x$增量，如果扫描线时从低到高，<strong>则$\Delta x$就是斜率的倒数</strong></li>
<li>$y_{max}$：该边所交的最高扫描线</li>
</ul>
</li>
</ul>
</li>
<li>新边表 <strong>(NET, New Egde Table)</strong><ul>
<li>存放在该扫描线第一次出现的边<ul>
<li>若某边的较低端点为$y<em>{min}$，则该边就放在扫描线$y</em>{min}$的新边表中</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>算法大致过程：初始化新边表，对于每条扫描线，从新边表对应的位置中取出边，插入排序到活性边表中，然后将活性边表内的节点两两配对，得到着色区间，最后将活性边表的$x$值递增、同时去掉不需要的边</li>
</ul>
<p><strong>边缘填充法</strong></p>
<ul>
<li>基本思想<ul>
<li>对于扫描线和各多边形的交点，将该扫描线上交点右方的所有像素颜色求补</li>
<li>两种实现方式<ul>
<li>以扫描线为中心的边缘填充</li>
<li>以边为中心的边缘填充</li>
</ul>
</li>
<li>引入栅栏提高效率</li>
</ul>
</li>
</ul>
<p><strong>边界标志法</strong></p>
<ul>
<li>基本思想<ul>
<li>对多边形的每条边进行直线扫描转换，也即对多边形边界所经过的像素打上标志</li>
<li>采用和扫描线算法类似的方法进行着色</li>
<li>使用一个布尔量inside来指示当前点是否在多边形内的状态</li>
</ul>
</li>
<li>需要注意两条边的交点计数次数</li>
</ul>
<h3 id="Lecture-4-裁剪"><a href="#Lecture-4-裁剪" class="headerlink" title="Lecture 4 裁剪"></a>Lecture 4 裁剪</h3><p>裁剪：确定图形中哪些部分落在显示区之内，哪些落在显示区之外，以便只显示落在显示区内的那部分图形</p>
<ul>
<li>裁剪窗口、裁剪对象</li>
<li>裁剪的时机（点阵图形 OR <strong>参数图形</strong>）</li>
</ul>
<h4 id="直线段裁剪"><a href="#直线段裁剪" class="headerlink" title="直线段裁剪"></a>直线段裁剪</h4><p>算法：</p>
<ul>
<li>直接求交法</li>
<li>Cohen-Sutherland裁剪</li>
<li>中点分割裁剪算法</li>
<li>Nicholl-Lee-Nicholl算法</li>
<li>梁有栋—Barssky算法</li>
<li>Lyrus-Beck算法</li>
</ul>
<p><strong>直接求交法</strong></p>
<ul>
<li>基本思想<ul>
<li>线段的两个端点都在窗口内，线段必定位于窗口内，即可见</li>
<li>线段的两个端点同时位于窗口左侧、右侧、上侧或下侧，线段必定位于窗口外，不可见</li>
<li>线段与窗口各边求交，得到可见部分</li>
</ul>
</li>
<li>算法过程<ul>
<li>过$P_1(x_1, y_1), P_2(x_2, y_2)$的直线$y = k(x - x_1) + y_1, k = (y_2 - y_1)/(x_2 - x_1)$</li>
<li>直线与窗口各边所在直线交点<ul>
<li>左：$x=x_L, y=k(x_L - x_1) + y_1, k\neq\infty$</li>
<li>右：$x=x_R, y=k(x_R - x_1) + y_1, k\neq\infty$</li>
<li>上：$y=y_T, x = (y_T - y) / k + x_1, k\neq 0$</li>
<li>下：$y=y_B, x = (y_B - y) / k + x_1, k\neq 0$</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Cohen-Sutherland裁剪</strong></p>
<p><strong>中点分割裁剪算法</strong></p>
<p><strong>Nicholl-Lee-Nicholl算法</strong></p>
<p><strong>Lyrus-Beck算法</strong></p>
<h4 id="多边形裁剪"><a href="#多边形裁剪" class="headerlink" title="多边形裁剪"></a>多边形裁剪</h4><p>算法</p>
<ul>
<li>Sutherland-Hodgeman算法</li>
<li><strong>Weiler-Athenton算法</strong></li>
</ul>
<p><strong>Weiler-Athenton算法</strong></p>
<ul>
<li>裁剪窗口，被裁剪多边形可以是任意多边形：凸、凹、带内环。裁剪窗口和被裁多边形地位对等</li>
<li>被裁剪多边形为<strong>主多边形</strong>，裁剪窗口为<strong>裁剪多边形</strong>。同时约定顶点序列方向：外环为逆时针，内环为顺时针，这样便于之后判断入点出点</li>
<li>基本思想<ul>
<li>裁剪结果区域由主多边形的部分边界和裁剪多边形的部分边界共同组成</li>
<li>在交点处，边界发生交替</li>
<li>交点成对出现</li>
<li>入点：裁剪多边形外部$\rightarrow$裁剪多边形内部</li>
<li>出点：裁剪多边形内部$\rightarrow$裁剪多边形外部</li>
<li><img src="/Users/lijx/Library/Application Support/typora-user-images/image-20221006212118852.png" alt="image-20221006212118852" style="zoom:50%;" /></li>
</ul>
</li>
<li>算法流程<ul>
<li>建立主多边形和裁剪多边形的顶点表</li>
<li>求交点、归类，并按顺序插入到顶点表中，在两个表的相应顶点间建立双向指针，便于之后查找<ul>
<li>判断出点、入点的方式：对两条相交的线段的向量求叉积（按照顶点遍历顺序得到向量），根据叉积结果的正负值来判断出点、入点，<strong>坐标系为左手系和右手系的结果恰好相反。</strong></li>
</ul>
</li>
<li>裁剪过程<ul>
<li>如果还有未跟踪过的交点，创建一个新的多边形，同时任取一个作为起点，加入多边形的顶点表。否则算法结束</li>
<li>如果该交点为入点，则在主多边形的顶点表中跟踪，取下一个节点。如果为出点，则在裁剪多边形的顶点表中跟踪。如果为多边形顶点，则保持当前跟踪的顶点表</li>
<li>找到一个环后，得到一个多边形</li>
</ul>
</li>
</ul>
</li>
</ul>
<h3 id="Lecture-5-图形变换与投影"><a href="#Lecture-5-图形变换与投影" class="headerlink" title="Lecture 5 图形变换与投影"></a>Lecture 5 图形变换与投影</h3><ul>
<li>齐次坐标与二维图形的几何变换</li>
<li>二维图形的显示流程</li>
<li>三维图形的几何变换</li>
<li>投影变换</li>
<li>三维图形的显示流程</li>
</ul>
<h3 id="Lecture-6-消隐"><a href="#Lecture-6-消隐" class="headerlink" title="Lecture 6 消隐"></a>Lecture 6 消隐</h3><h3 id="Lecture-7-消隐2"><a href="#Lecture-7-消隐2" class="headerlink" title="Lecture 7 消隐2"></a>Lecture 7 消隐2</h3><h3 id="Lecture-8"><a href="#Lecture-8" class="headerlink" title="Lecture 8"></a>Lecture 8</h3>]]></content>
  </entry>
  <entry>
    <title>GAMES101</title>
    <url>/2022/09/19/GAMES101/</url>
    <content><![CDATA[<h2 id="GAMES101学习笔记"><a href="#GAMES101学习笔记" class="headerlink" title="GAMES101学习笔记"></a>GAMES101学习笔记</h2><h3 id="Lecture-1-Introduction"><a href="#Lecture-1-Introduction" class="headerlink" title="Lecture 1: Introduction"></a>Lecture 1: Introduction</h3><p>课程内容：</p>
<ul>
<li>Rasterization 光栅化</li>
<li>Curves and Meshes 曲线和曲面</li>
<li>Ray Tracing 光线追踪</li>
<li>Animation / Simulation 动画 / 模拟</li>
</ul>
<h4 id="Rasterization"><a href="#Rasterization" class="headerlink" title="Rasterization"></a>Rasterization</h4><ul>
<li>将几何单元投影到屏幕</li>
<li>将投影结果打散成像素</li>
</ul>
<h4 id="Curves-and-Meshes"><a href="#Curves-and-Meshes" class="headerlink" title="Curves and Meshes"></a>Curves and Meshes</h4><p>在计算机图形学中，如何表示几何图形</p>
<h4 id="Ray-Tracing"><a href="#Ray-Tracing" class="headerlink" title="Ray Tracing"></a>Ray Tracing</h4><ul>
<li>Shoot rays from the camera through each pixel<ul>
<li>Calculate intersection and shading</li>
<li>Continue to bounce the rays till they hit light sources</li>
</ul>
</li>
<li>Gold standard in Animations / Movies</li>
</ul>
<h4 id="Animation-Simulation"><a href="#Animation-Simulation" class="headerlink" title="Animation / Simulation"></a>Animation / Simulation</h4><h3 id="Lecture-2-Review-of-Linear-Algebra"><a href="#Lecture-2-Review-of-Linear-Algebra" class="headerlink" title="Lecture 2: Review of Linear Algebra"></a>Lecture 2: Review of Linear Algebra</h3><p><strong>向量、向量内积</strong></p>
<p><strong>向量叉积: $ a\times b = -b \times a$且$||a \times b|| = ||a||||b||\sin\psi$，其中$\psi$为a与b的夹角，且$a\times b$垂直于a、b构成的平面，方向遵循右手定则</strong></p>
<script type="math/tex; mode=display">
a \times b = 
\begin{pmatrix}
y_az_b - y_bz_a \\
z_ax_b - x_az_b \\
x_ay_b - y_ax_b
\end{pmatrix}
= A^*b = 
\begin{pmatrix}
0 & -z_a & y_a \\
z_a & 0 & -x_a \\
-y_a & x_a & 0
\end{pmatrix}
\begin{pmatrix}
x_b \\
y_b \\
z_b
\end{pmatrix}</script><p>叉积作用：判断两个向量左/右关系，判断点在三角形的内部/外部。</p>
<p>按一个方向遍历三角形的顶点并取叉积，只有三个叉积结果同时为正或同时为负时，该点在三角形内部</p>
<h3 id="Lecture-3-Transformation"><a href="#Lecture-3-Transformation" class="headerlink" title="Lecture 3: Transformation"></a>Lecture 3: Transformation</h3><ul>
<li>为什么要学习变换？在图形学当中视角的转换、绘制当中都需要变换</li>
<li><strong>2D transformations</strong><ul>
<li>使用矩阵表示变换</li>
<li>Rotation, scale, shear 旋转、缩放、切变</li>
</ul>
</li>
<li><strong>Homogeneous coordinates</strong> 齐次坐标</li>
</ul>
<p><strong>Scale Matrix</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
s_x & 0 \\
0 & s_y
\end{pmatrix}</script><p><strong>Reflection Matrix</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
-1 & 0 \\
0 & 1
\end{pmatrix}</script><p><strong>Shear Matrix</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
1 & a \\
0 & 1 \\
\end{pmatrix}</script><p><img src="/Users/lijx/Library/Application Support/typora-user-images/image-20220919212647209.png" alt="image-20220919212647209" style="zoom:30%;" /></p>
<p><strong>Rotation Matrix</strong></p>
<p>逆时针旋转$\theta$</p>
<script type="math/tex; mode=display">
\begin{pmatrix}
\cos\theta & -\sin\theta \\
\sin\theta & \cos\theta
\end{pmatrix}</script><h4 id="Homogeneous-Coordinates"><a href="#Homogeneous-Coordinates" class="headerlink" title="Homogeneous Coordinates"></a>Homogeneous Coordinates</h4><p>为什么要引入齐次坐标？平移变换无法用二维坐标的线性变化来表示，在齐次坐标下，这些变换都可以用线性变换表示</p>
<ul>
<li>2D point = $(x, y, 1)^T$</li>
<li>2D vector = $(x, y, 0)^T$</li>
</ul>
<p><strong>齐次坐标下的平移变换矩阵</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
1 & 0 & t_x \\
0 & 1 & t_y \\
0 & 0 & 1
\end{pmatrix}</script><p><strong>Affine Transformations 仿射变换 = linear map + translation</strong></p>
<script type="math/tex; mode=display">
\begin{pmatrix}
x'\\
y'
\end{pmatrix} = 
\begin{pmatrix}
a & b \\
c & d 
\end{pmatrix}
\begin{pmatrix}
x \\
y
\end{pmatrix} + 
\begin{pmatrix}
t_x\\
t_y
\end{pmatrix}</script><script type="math/tex; mode=display">
\begin{pmatrix}
x'\\
y'\\
1
\end{pmatrix} =
\begin{pmatrix}
a & b & t_x \\
c & d & t_y \\
0 & 0 & 1 
\end{pmatrix}
\begin{pmatrix}
x\\
y\\
1
\end{pmatrix}</script><p>线性变换的逆变换的变换矩阵是矩阵的逆。</p>
<h3 id="Lecture-4-Transformation-Cont"><a href="#Lecture-4-Transformation-Cont" class="headerlink" title="Lecture 4: Transformation Cont."></a>Lecture 4: Transformation Cont.</h3><ul>
<li>3D transformations</li>
<li>Viewing (观测) transformation<ul>
<li>View (视图) / Camera transformation</li>
<li>Projection (投影) transformation<ul>
<li>Orthographic (正交) projection</li>
<li>Perspective (透视) projection</li>
</ul>
</li>
</ul>
</li>
</ul>
<p><strong>Rotation around x-, y-, or z-axis</strong></p>
<script type="math/tex; mode=display">
R_x(\alpha) = \begin{pmatrix}
1 & 0 & 0 & 0 \\
0 & \cos\alpha & -\sin\alpha & 0 \\
0 & \sin\alpha & \cos\alpha & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}</script><script type="math/tex; mode=display">
R_y(\alpha) = \begin{pmatrix}
\cos\alpha & 0 & \sin\alpha & 0 \\
0 & 1 & 0 & 0 \\
-\sin\alpha & 0 & \cos\alpha & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}</script><script type="math/tex; mode=display">
R_z(\alpha) = \begin{pmatrix}
\cos\alpha & -\sin\alpha & 0 & 0 \\
\sin\alpha & \cos\alpha & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1
\end{pmatrix}</script><h4 id="Rodrigues’-Rotation-Formula"><a href="#Rodrigues’-Rotation-Formula" class="headerlink" title="Rodrigues’ Rotation Formula"></a>Rodrigues’ Rotation Formula</h4><p><strong>Rotation by angle $\alpha$ around axis n</strong></p>
<script type="math/tex; mode=display">
\pmb{R}(\pmb{n}, \alpha) = \cos(\alpha)\pmb{I} + (1 - \cos\alpha)\pmb{nn^T} + \sin\alpha
\begin{pmatrix}
0 & -n_z & n_y \\
n_z & 0 & -n_x \\
-n_y & n_x & 0
\end{pmatrix}</script><h4 id="Projection-Transformation"><a href="#Projection-Transformation" class="headerlink" title="Projection Transformation"></a>Projection Transformation</h4><p><strong>Orthographic Projection</strong></p>
<script type="math/tex; mode=display">
M_{ortho} = 
\begin{pmatrix}
\frac{2}{r-l} & 0 & 0 & 0 \\
0 & \frac{2}{t-b} & 0 & 0 \\
0 & 0 & \frac{2}{n-f} & 0 \\
0 & 0 & 0 & 1 \\
\end{pmatrix}
\begin{pmatrix}
1 & 0 & 0 & -\frac{r+l}{2} \\
0 & 1 & 0 & -\frac{t+b}{2} \\
0 & 0 & 1 & -\frac{n+f}{2} \\
0 & 0 & 0 & 1 \\
\end{pmatrix}</script><p><strong>Perspective Projection</strong></p>
<script type="math/tex; mode=display">
M_{persp \rightarrow ortho} =
\begin{pmatrix}
n & 0 & 0 & 0\\
0 & n & 0 & 0\\
0 & 0 & n+f & -nf \\
0 & 0 & 1 & 0
\end{pmatrix}</script><script type="math/tex; mode=display">
M_{persp} = M_{ortho}M_{persp\rightarrow ortho}</script>]]></content>
  </entry>
  <entry>
    <title>Machine Learning</title>
    <url>/2022/08/19/Machine-Learning/</url>
    <content><![CDATA[<h2 id="一些经典机器学习算法"><a href="#一些经典机器学习算法" class="headerlink" title="一些经典机器学习算法"></a>一些经典机器学习算法</h2><h3 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h3><p>问题定义：给定一个观测数据集$\pmb{(x_1, x_2, …, x_n)}$，k-means的目标是将这n个数据点分为k个类，即$S = {S_1, S_2, …,S_k}$，使得within-cluster sum of squares（WCSS）最小</p>
<script type="math/tex; mode=display">
\arg\min_{S} \sum_{i=1}^k\sum_{x\in S_i}|| \pmb{x - \mu_i}||^2</script><p>直接求解比较困难，通常使用启发式算法进行求解，算法流程如下</p>
<ol>
<li>随机初始化$\pmb{\mu_1, \mu_2, …, \mu_k}$</li>
<li>对每个数据点$\pmb{x}$，将其类标签设为$\arg \min_{k} ||\pmb{x - \mu_k}||^2$</li>
<li>重新计算类中心点，即$\pmb{\mu<em>k} = \frac{\sum</em>{x \in S_k} x}{|S_k|}$</li>
<li>重复2、3，直到收敛</li>
</ol>
<p>很容易就能证明，第2、3步都会使得目标函数减小，因此重复迭代能不断减小目标函数的值，使得聚类效果更好。</p>
<p>缺点：得到的聚类集合通常是凸集合，对一些狭长的分布不友好；初始化的值会对最终的聚类结果有很大影响<br><span id="more"></span></p>
<h3 id="Guassian-Mixture-Model"><a href="#Guassian-Mixture-Model" class="headerlink" title="Guassian Mixture Model"></a>Guassian Mixture Model</h3><ul>
<li>类概率：$\pi = (\pi_1, \pi_2, …, \pi_k)$</li>
<li>类均值：$\pmb{\mu = (\mu_1, \mu_2, …, \mu_k)}$</li>
<li>类协方差：$\pmb{\Sigma = (\Sigma_1, \Sigma_2, …, \Sigma_k)}$</li>
</ul>
<p>记$\theta = (\pi, \pmb{\mu, \Sigma})$</p>
<p>问题定义：给定一个观测数据集$\pmb{X} = \pmb{(x_1, x_2, …, x_n)}$。假设数据是由高斯混合分布生成，对$\theta$做极大似然估计。</p>
<p>高斯混合分布的密度函数为</p>
<script type="math/tex; mode=display">
p(\pmb{x}) = \sum_{i=1}^k \pi_i \mathcal{N}(\pmb{x|\mu_i, \Sigma_k})</script><p>首先写出似然函数</p>
<script type="math/tex; mode=display">
\log p(\pmb{X} | \theta) = \log \prod_{i=1}^n p(\pmb{x}_i|\theta) = \sum_{i=1}^n \log p(\pmb{x_i} | \theta) = \sum_{i=1}^n \log \sum_{j=1}^k \pi_j \mathcal{N}(\pmb{x_j|\mu_i, \Sigma_k})</script><p>直接对似然函数求最大值点非常困难，所以在此使用EM算法进行求解</p>
<script type="math/tex; mode=display">
\begin{align}
\log p(\pmb{x}|\theta) &= \log\left[\sum_{z}p(\pmb{x}, z|\theta)\right] \\
&= \log\left[\sum_{z}q(z)\frac{p(\pmb{x}, z|\theta)}{q(z)}\right] \\
&\geq \sum_z q(z) \log(\frac{p(\pmb{x}, z|\theta)}{q(z)}) := \mathcal{L}(q, \theta)
\end{align}</script><p>EM算法：$\arg\max_{\theta}\left[\max_q\mathcal{L}(q, \theta)\right]$，迭代这个过程直到收敛，可以证明每一次迭代，似然函数都会增大</p>
<script type="math/tex; mode=display">
\mathcal{L}(q, \theta) = -KL[q(z) \ || \ p(z|\pmb{x}, \theta)] + \log p(\pmb{x}|\theta)</script><ul>
<li>Expectation Step<ul>
<li>$q^\star(z) = p(z|x, \theta_{old})$</li>
<li>令$J(\theta) := \mathcal(q^\star, \theta) = \sum_{z} q^\star(z) \log\left(\frac{p(x,z|\theta)}{q^\star(z)}\right)$</li>
</ul>
</li>
<li>Maximization Step<ul>
<li>$\theta<em>{new} = \arg\max</em>{\theta} J(\theta)$</li>
</ul>
</li>
</ul>
<img src="/2022/08/19/Machine-Learning/EM-GMM.png" class=""> ]]></content>
  </entry>
  <entry>
    <title>Reinforcement Learning</title>
    <url>/2022/06/20/Reinforcement-Learning/</url>
    <content><![CDATA[<h3 id="有限马尔可夫决策过程（Markov-Decision-Process-MDP）"><a href="#有限马尔可夫决策过程（Markov-Decision-Process-MDP）" class="headerlink" title="有限马尔可夫决策过程（Markov Decision Process, MDP）"></a>有限马尔可夫决策过程（Markov Decision Process, MDP）</h3><p>一个MDP可以用一个四元组表示$M = &lt; S, A, P, R&gt;$，其中</p>
<ul>
<li><em>S</em>: 一个有限的状态集合，$S = {s_1, s_2, \cdots, s_n}$</li>
<li><em>A</em>: 一个有限的动作集合，$A = {a_1, a_2, \cdots, a_m}$</li>
<li><em>P</em>: 状态转移概率，$P<em>{ss’}^a = Pr\left[S</em>{t+1} = s’ | S_t = s, A_t = a\right]$</li>
<li><em>R</em>: Reward函数，$R<em>s^a = E\left[R</em>{t+1} | S_t = s, A_t = a\right]$</li>
</ul>
<p>定义Return为</p>
<script type="math/tex; mode=display">
G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2R_{t+3} + \cdots = \sum_{k=0}^\infty \gamma^k R_{t+k+1}</script><p>性质：$G<em>t = R</em>{t+1} + \gamma G_{t+1}$，其中$0 \leq \gamma \leq 1$, 当$\gamma=1$时，需要确保每条序列都终止。<br><span id="more"></span></p>
<h4 id="Value-Function"><a href="#Value-Function" class="headerlink" title="Value Function"></a>Value Function</h4><p>令策略为$\pi$。</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \mathbb{E_\pi}\left[G_t|S_t = s\right] \\
    &q_{\pi}(s, a) = \mathbb{E_\pi}\left[G_t \ | \ S_t=s, A_t = a \right]
\end{align*}</script><h4 id="Bellman-Equation"><a href="#Bellman-Equation" class="headerlink" title="Bellman Equation"></a>Bellman Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_\pi(s')) \\
    &q_{\pi}(s,a) = R_s^a + \gamma\sum_{s'\in S} P_{ss'}^a \sum_{a' \in A} \pi(a'|s')q_\pi(s', a')
\end{align*}</script><p>写成期望形式</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_\pi(s) = \mathbb{E_\pi}[R_{t+1} + \gamma v_\pi(S_{t+1})| S_t = s] \\
    &q_{\pi}(s, a) = \mathbb{E}[R_{t+1} + \gamma q_\pi(S_{t+1}, A_{t+1})|S_t = s, A_t = a]
\end{align*}</script><p>写成矩阵形式</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n)
\end{bmatrix} = 
\begin{bmatrix}
    \mathcal{R}_{s_1} \\
    \vdots \\
    \mathcal{R}_{s_n}
\end{bmatrix} + \gamma
\begin{bmatrix}
    P_{s_1s_1} & \cdots & P_{s_1s_n} \\
    \vdots & & \vdots \\
    P_{s_ns_1} & \cdots & P_{s_ns_n} \\
\end{bmatrix}
\begin{bmatrix}
    v(s_1) \\
    \vdots \\
    v(s_n) 
\end{bmatrix}</script><p>其中$\mathcal{R}<em>s = \mathbb{E}[R</em>{t+1} | S<em>t = s]$, $P</em>{ss’} = Pr[S_{t+1}=s’ | S_t = s]$。求解复杂度为$O(n^3)$，通常不会使用。</p>
<h4 id="Optimal-Policies-and-Optimal-Value-Function"><a href="#Optimal-Policies-and-Optimal-Value-Function" class="headerlink" title="Optimal Policies and Optimal Value Function"></a>Optimal Policies and Optimal Value Function</h4><p>$\pi \geq \pi’$当且仅当对于任意s，都满足$v<em>\pi(s) &gt; v</em>{\pi’}(s)$</p>
<p>记最优策略为$\pi_{*}$，则有</p>
<script type="math/tex; mode=display">
\begin{align*}
    &v_{\pi_*}(s) = \max_{\pi} v_{\pi}(s) \\
    &q_{\pi_*}(s, a) = \max_{\pi} q_{\pi}(s, a)
\end{align*}</script><h4 id="Bellman-Optimal-Equation"><a href="#Bellman-Optimal-Equation" class="headerlink" title="Bellman Optimal Equation"></a>Bellman Optimal Equation</h4><script type="math/tex; mode=display">
\begin{align*}
    &v_*(s) = \max_{a} q_{\pi_*}(s, a) \\
    &q_*(s, a) = R_s^a + \gamma\sum_{s' \in S} P_{ss'}^a v_*(s')
\end{align*}</script><h3 id="使用动态规划方法求解MDP"><a href="#使用动态规划方法求解MDP" class="headerlink" title="使用动态规划方法求解MDP"></a>使用动态规划方法求解MDP</h3><h4 id="策略评估（Policy-Evaluation）"><a href="#策略评估（Policy-Evaluation）" class="headerlink" title="策略评估（Policy Evaluation）"></a>策略评估（Policy Evaluation）</h4><p>策略评估指的是给定一个策略$\pi$，求出其价值函数$v_\pi$的过程。根据Bellman Equation，可以使用如下更新公式</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) = \sum_{a\in A} \pi(a|s) (R_s^a + \gamma\sum_{s'\in S}P_{ss'}^av_k(s'))</script><p>具体计算时，既可以采用两个数组分别记录$v<em>{k+1}, v_k$，也可以只用一个数组就地更新，两种方式都可以收敛到$v</em>\pi$, 但收敛速度会有所不同。</p>
<h4 id="策略提升（Policy-Improvement）"><a href="#策略提升（Policy-Improvement）" class="headerlink" title="策略提升（Policy Improvement）"></a>策略提升（Policy Improvement）</h4><p>策略提升指的是根据给定策略$\pi$，得到一个新的策略$\pi’$，使得$\pi’ \geq \pi$。</p>
<p>使用如下方式对策略进行更新</p>
<script type="math/tex; mode=display">
\begin{align*}
    \pi'(s) &:= \text{argmax}_{a} q_\pi(s, a) \\
            &= \text{argmax}_{a} R_s^a + \gamma \sum_{s' \in S}P_{ss'}^av_\pi(s')
\end{align*}</script><p>上述方法是有效的，证明如下，假设$\pi$和$\pi’$都是确定性策略，且对于任意$s\in S$，都有</p>
<script type="math/tex; mode=display">
    q_\pi(s, \pi'(s)) \geq v_\pi(s)</script><p>则</p>
<script type="math/tex; mode=display">
\begin{align*}
    v_\pi(s) &\leq q_\pi(s, \pi'(s)) \\
             &\leq \mathbb{E}[R_{t+1} + \gamma v_\pi(S_{t+1}) | S_t=t, A_t=\pi'(s)] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma q_\pi(S_{t+1}, \pi'(S_{t+1})) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma\mathbb{E}[R_{t+2} + \gamma v_\pi(S_{t+2})| S_{t+1}, A_{t+1}=\pi'(S_{t+1})] | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 v_\pi(S_{t+2}) | S_t=s] \\
             &\leq \mathbb{E_{\pi'}}[R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+2} + \gamma^3 R_{t+3} + \cdots | S_t=s, A_t=\pi'(s)] \\
             &= v_{\pi'}(s)
\end{align*}</script><h4 id="策略迭代（Policy-Iteration）"><a href="#策略迭代（Policy-Iteration）" class="headerlink" title="策略迭代（Policy Iteration）"></a>策略迭代（Policy Iteration）</h4><ol>
<li>策略评估</li>
<li>策略提升</li>
<li>重复1、2直到收敛</li>
</ol>
<h4 id="价值迭代（Value-Iteration）"><a href="#价值迭代（Value-Iteration）" class="headerlink" title="价值迭代（Value Iteration）"></a>价值迭代（Value Iteration）</h4><p>使用Bellman Optimal Equation直接计算出$v_{*}(s)$, 迭代公式为</p>
<script type="math/tex; mode=display">
    v_{k+1}(s) := \max_a R_s^a + \gamma \sum_{s' \in S} P_{ss'}^av_k(s)</script><p>令$\pi_{*}(s)$为</p>
<script type="math/tex; mode=display">
   \pi_{*}(s) = \text{argmax}_a q_*(s, a)</script><h3 id="Temporal-Difference-Learning-时序差分学习"><a href="#Temporal-Difference-Learning-时序差分学习" class="headerlink" title="Temporal Difference Learning (时序差分学习)"></a>Temporal Difference Learning (时序差分学习)</h3><p>与蒙特卡洛方法一致，时序差分方法也可以直接从与环境互动的经验中学习策略，而不需要构建关于环境动态特性的模型。与动态规划相一致的是，时序差分方法也可以基于已得到的其他状态的估计值来更新当前状态的价值函数。在时序差分学习算法中，n-步算法将TD方法和MC方法联系在一起、TD（$\lambda$）能无缝地统一TD方法和MC方法。</p>
<p>MC方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[G_t - V(S_t)\right]</script><p>TD方法更新公式：</p>
<script type="math/tex; mode=display">
V(S_t) \gets V(S_t) + \alpha \left[R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]</script><p>其中$\delta<em>t = R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t)$也称作TD error。</p>
<h4 id="TD-0"><a href="#TD-0" class="headerlink" title="TD(0)"></a>TD(0)</h4><p>更新公式：$V(S<em>t) \gets V(S_t) + \alpha \left[R</em>{t+1} + \gamma V(S_{t+1}) - V(S_t) \right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/TD(0).png" class="">
<h4 id="Sarsa"><a href="#Sarsa" class="headerlink" title="Sarsa"></a>Sarsa</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma Q(S<em>{t+1}, A</em>{t+1}) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/sarsa.png" class="">
<h4 id="Q-Learning"><a href="#Q-Learning" class="headerlink" title="Q-Learning"></a>Q-Learning</h4><p>更新公式：$Q(S<em>t, A_t) \gets Q(S_t, A_t) + \alpha\left[R</em>{t+1} + \gamma \max<em>{a’}Q(S</em>{t+1}, a’) - Q(S_t, A_t)\right]$</p>
<img src="/2022/06/20/Reinforcement-Learning/q-learning.png" class="">
<h3 id="函数近似"><a href="#函数近似" class="headerlink" title="函数近似"></a>函数近似</h3><p>对价值函数进行参数化建模，用参数为$\theta$的函数$\hat{v}(s, \pmb{w})$去近似$v<em>\pi(s)$。指定一个状态的分布$\mu(s) \geq 0, \sum</em>{s} \mu(s) = 1$来表示我们对于每个状态s的误差的重视程度。一个自然的目标函数为均方价值误差，记为$VE$</p>
<script type="math/tex; mode=display">
VE(\pmb{w}) = \sum_{s\in S}\mu(s)\left[v_\pi(s) - \hat{v}(s, \pmb{w})\right]^2</script><p>假设在样本中的分布$\mu$和VE中的分布相同。在这种情况下，一个好的策略就是尽量减少观测到的样本的误差。使用SGD方法进行优化</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{w}_{t+1} &= \pmb{w}_t - \frac{1}{2}\alpha\nabla\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]^2 \\
&= \pmb{w}_t - \alpha\left[v_\pi(S_t) - \hat{v}(S_t, \pmb{w}_t)\right]\nabla\hat{v}(S_t, \pmb{w}_t)
\end{align*}</script><p>由于我们无法得到$v<em>\pi(S_t)$，所以使用它的一个随机近似$U_t$替代，如果$U_t$是一个无偏估计，那么收敛性是有保证的。可以取$U_t$为$G_t$，这种方法被称做梯度方法。如果采用自举估计值，取$U_t$为$R</em>{t+1}+\hat{v}(S_{t+1}, \pmb{w})$，则没有足够的收敛性保证，但在大多数情况下，都会收敛，这种方法称为半梯度方法。</p>
<h3 id="Policy-Gradient"><a href="#Policy-Gradient" class="headerlink" title="Policy Gradient"></a>Policy Gradient</h3><p>基于动作价值函数的方法，都是先学习动作价值函数，然后根据估计的动作价值函数选择动作，策略的获得依赖于价值函数。策略梯度方法对策略进行参数化建模，通过梯度上升的方式直接对策略进行学习。用$\pmb{\theta}$表示策略的参数向量。</p>
<p>策略参数的学习方法都基于某种性能度量$J(\pmb{\theta})$的梯度，性能度量可以是从初始状态s到终止状态s’的return的期望。在能够分出episode和连续型的条件下，性能度量会有所不同。在得到梯度后，通过梯度上升，不断对策略的参数进行优化</p>
<script type="math/tex; mode=display">
\pmb{\theta_{t+1}} = \pmb{\theta_{t}} + \alpha \widehat{\nabla J(\pmb{\theta_{t}})}</script><p>其中，$\widehat{\nabla J(\pmb{\theta<em>{t}})}$是对$\nabla J(\pmb{\theta</em>{t}})$的随机估计。</p>
<h4 id="策略梯度定理"><a href="#策略梯度定理" class="headerlink" title="策略梯度定理"></a>策略梯度定理</h4><p>在分幕式情况下策略梯度定理表达式如下</p>
<script type="math/tex; mode=display">
\nabla J(\pmb{\theta}) \propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta})</script><p>其中，$\mu(s)$表示在使用策略$\pi$的条件下，s出现在轨迹中的概率。</p>
<h4 id="REINFORCE"><a href="#REINFORCE" class="headerlink" title="REINFORCE"></a>REINFORCE</h4><p>由于$\mu(s)$是将目标策略$\pi$下每个状态出现的频率作为加权系数的求和项，如果按照策略$\pi$执行，则状态将按这个比例出现。假设$\gamma$为1即没有折扣，根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \mathbb{E}_\pi\left[\sum_{a} q_\pi(S_t, a)\nabla \pi (a|S_t, \pmb{\theta})\right] \\
&= \mathbb{E}_\pi\left[\sum_{a} \pi(a|S_t, \pmb{\theta})q_\pi(S_t, a)\frac{\nabla \pi (a|S_t, \pmb{\theta})}{\pi(a|S_t, \pmb{\theta})}\right] \\
&= \mathbb{E}_\pi\left[q_\pi(S_t, A_t)\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{用采样$A_t \sim \pi$替换$a$} \\
&= \mathbb{E}_\pi\left[G_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta})}{\pi(A_t|S_t, \pmb{\theta})}\right] \tag{$\mathbb{E}_\pi\left[G_t|S_t, A_t\right] = q_\pi(S_t, A_t)$} \\
&= \mathbb{E}_\pi\left[G_t\nabla \ln \pi (A_t|S_t, \pmb{\theta})\right] \tag{$\nabla\ln x = \frac{\nabla x}{x}$}
\end{align*}</script><p>则可以得到REINFORCE算法<br><img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient.png" class=""></p>
<h4 id="REINFORCE-with-baseline"><a href="#REINFORCE-with-baseline" class="headerlink" title="REINFORCE with baseline"></a>REINFORCE with baseline</h4><p>根据策略梯度定理</p>
<script type="math/tex; mode=display">
\begin{align*}
\nabla J(\pmb{\theta}) &\propto \sum_s \mu(s) \sum_{a} q_\pi(s, a)\nabla \pi (a|s, \pmb{\theta}) \\
&= \sum_s \mu(s) \sum_{a} (q_\pi(s, a)-b(s))\nabla \pi (a|s, \pmb{\theta})
\end{align*}</script><p>由此导出更新公式</p>
<script type="math/tex; mode=display">
\pmb{\theta}_{t+1} = \pmb{\theta}_{t} + \alpha\left(G_t - b(S_t)\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}</script><p>其中的$b(s)$可以是不随动作$a$变化的任意函数。因为这个baseline也可以是常量0，所以上式是REINFORCE算法更一般的推广。通常来说，加入baseline不会使更新的期望值发生变化，但是对方差会有很大影响。</p>
<img src="/2022/06/20/Reinforcement-Learning/Policy-Gradient-w-baseline.png" class="">
<h4 id="Actor-Critic"><a href="#Actor-Critic" class="headerlink" title="Actor-Critic"></a>Actor-Critic</h4><p>尽管带基线的强化学习方法既学习了一个策略函数也学习了一个状态价值函数，我们也不认为它是一种AC方法，因为它的状态价值函数仅被用作baseline，而不是作为一个Critic，也即没有被用于自举操作。只有当采用自举法时，才会出现依赖于函数逼近质量的偏差和渐进性收敛。引入自举，往往能减小方差和加速收敛。</p>
<p>考虑单步AC方法，使用单步回报代替$G_t$，则得到如下更新公式</p>
<script type="math/tex; mode=display">
\begin{align*}
\pmb{\theta}_{t+1} &= \pmb{\theta}_{t} + \alpha\left(G_{t:t+1} - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\left(R_{t+1} + \hat{v}(S_{t+1}, \pmb{w}) - \hat{v}_(S_t, \pmb{w})\right)\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)} \\
&= \pmb{\theta}_{t} + \alpha\delta_t\frac{\nabla \pi (A_t|S_t, \pmb{\theta}_t)}{\pi(A_t|S_t, \pmb{\theta}_t)}
\end{align*}</script><p>进而可以得到如下算法</p>
<img src="/2022/06/20/Reinforcement-Learning/Actor-Critic.png" class="">
<h3 id="Deep-Reinforcement-Learning-DRL"><a href="#Deep-Reinforcement-Learning-DRL" class="headerlink" title="Deep Reinforcement Learning (DRL)"></a>Deep Reinforcement Learning (DRL)</h3><h4 id="Deep-Q-Learning-DQN"><a href="#Deep-Q-Learning-DQN" class="headerlink" title="Deep Q-Learning (DQN)"></a>Deep Q-Learning (DQN)</h4><h4 id="Deep-Deterministic-Policy-Gradient-DDPG"><a href="#Deep-Deterministic-Policy-Gradient-DDPG" class="headerlink" title="Deep Deterministic Policy Gradient (DDPG)"></a>Deep Deterministic Policy Gradient (DDPG)</h4><h4 id="Trust-Region-Policy-Gradient-TRPO"><a href="#Trust-Region-Policy-Gradient-TRPO" class="headerlink" title="Trust Region Policy Gradient (TRPO)"></a>Trust Region Policy Gradient (TRPO)</h4><h4 id="Proximal-Policy-Gradient-PPO"><a href="#Proximal-Policy-Gradient-PPO" class="headerlink" title="Proximal Policy Gradient (PPO)"></a>Proximal Policy Gradient (PPO)</h4><h4 id="Soft-Actor-Critic-SAC"><a href="#Soft-Actor-Critic-SAC" class="headerlink" title="Soft Actor-Critic (SAC)"></a>Soft Actor-Critic (SAC)</h4><p>参考文献：</p>
<ol>
<li><a href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">《Reinforcement Learning: An Introduction》</a></li>
</ol>
]]></content>
  </entry>
  <entry>
    <title>Variational Inference</title>
    <url>/2022/07/10/Variational-Inference/</url>
    <content><![CDATA[<p>假设随机变量$X$和$Z$服从联合分布$p(x, z; \theta)$，其中$\theta$是分布的参数。我们可以得到$X$的观测值，但不能得到$Z$的观测值，即$X$为观测变量，$Z$为隐变量。给定$\theta$，并且有观测值$x$，我们希望得到$Z$的后验分布$p(z; x, \theta)$。通常这个后验分布无法直接计算，但可以通过一个变分分布$q(z)$近似。’<br><span id="more"></span></p>
<h3 id="Kullback-Leibler-divergence-KL散度"><a href="#Kullback-Leibler-divergence-KL散度" class="headerlink" title="Kullback-Leibler divergence (KL散度)"></a>Kullback-Leibler divergence (KL散度)</h3><p>对于离散分布$P$和$Q$，KL散度定义为</p>
<script type="math/tex; mode=display">
D_{KL}\left(P \ ||\ Q\right) = \sum_{x \in \mathcal{X}} P(x)\log\left(\frac{P(x)}{Q(x)}\right)</script><p>对于连续分布$P$和$Q$，KL散度定义为</p>
<script type="math/tex; mode=display">
D_{KL}\left(P \ ||\ Q\right) = \int_{-\infty}^{+\infty} P(x)\log\left(\frac{P(x)}{Q(x)}\right) dx</script><h3 id="Evidence-Lower-Bound-ELBO"><a href="#Evidence-Lower-Bound-ELBO" class="headerlink" title="Evidence Lower Bound (ELBO)"></a>Evidence Lower Bound (ELBO)</h3><p>evidence定义为$\log p(x; \theta)$。假设随机变量$Z$服从分布$q$，则可以推出evidence的一个下界。</p>
<script type="math/tex; mode=display">
\begin{align*}
\log p(x; \theta) &= \log \int p(x, z; \theta) dz \\
&= \log \int q(z) \frac{p(x, z; \theta)}{q(z)} dz \\
&= \log \mathbb{E}_{Z\sim q}\left[\frac{p(x, Z)}{q(Z)}\right] \\
&\geq \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z)}{q(Z)} \right] \tag{Jensen's Inequality} \\
\end{align*}</script><p><strong>Jenson’ s Inequality: </strong>$\mathbb{E}[\varphi(X)] \geq \varphi(\mathbb{E}[X])$，其中$\varphi$是一个凸函数。</p>
<p>将$\mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z)}{q(Z)}\right]$ 定义为<strong>ELBO</strong>，显然，ELBO和evidence之间存在一个gap，而这个gap变分分布$q(z)$和真实分布$p(z;x, \theta)$的KL散度，推导如下。</p>
<script type="math/tex; mode=display">
\begin{align*}
KL(q(z) \ || \ p(z|x, \theta)) &:= \mathbb{E}_{Z\sim q}\left[\log \frac{q(Z)}{p(Z; x, \theta)} \right] \\
&= \mathbb{E}_{Z\sim q}\left[\log q(Z) \right] - \mathbb{E}_{Z\sim q}\left[\log\frac{p(x, Z; \theta)}{p(x; \theta)} \right] \\
&= \mathbb{E}_{Z\sim q}\left[\log q(Z) \right] - \mathbb{E}_{Z\sim q}\left[\log p(x, Z; \theta) \right] + \mathbb{E}_{Z\sim q}\left[\log p(x; \theta) \right] \\
&= \log p(x; \theta) - \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z; \theta)}{q(Z)} \right] \\
&= evidence - ELBO
\end{align*}</script><h4 id="Variational-Inference"><a href="#Variational-Inference" class="headerlink" title="Variational Inference"></a>Variational Inference</h4><p>在得到观测数据$x$之后，我们希望做极大似然估计求$\theta$，但由于边缘分布$p(x; \theta) = \frac{p(x,z; \theta)}{p(z;x, \theta)}$无法直接计算，所以希望使用一个变分分布$q(z)$去近似$p(z; x, \theta)$，从而能够计算$p(x; \theta)$。为了使$q(z)$较好地近似$p(z; x, \theta)$，可以通过减小它们之间的KL散度实现，也即最大化ELBO。</p>
<h4 id="Expectation-Maximization-EM算法"><a href="#Expectation-Maximization-EM算法" class="headerlink" title="Expectation-Maximization (EM算法)"></a>Expectation-Maximization (EM算法)</h4><ul>
<li><strong>Expectation Step: </strong>$q^* = \arg\max<em>{q} ELBO =  \arg\max</em>{q} \mathbb{E}_{Z\sim q}\left[\log \frac{p(x, Z; \theta)}{q(Z)} \right]$</li>
<li><strong>Maximization Step: </strong>$\theta<em>{new} = \arg \max</em>{\theta} ELBO = \arg\max<em>{\theta}\mathbb{E}</em>{Z\sim {q^\star}}\left[\log \frac{p(x, Z; \theta)}{q^\star(Z)} \right]$</li>
<li>重复这个过程直到收敛</li>
</ul>
<p>E步相当于在给定$\theta$的情况下，找到一个最优的$q(z)$，去逼近$p(z;x, \theta)$，使得通过变分得到估计尽量准确；M步相当于，在得到了一个较为准确的估计的条件下，做极大似然估计。</p>
<p>参考文章：<a href="https://mbernste.github.io/posts/elbo/">ELBO</a></p>
]]></content>
  </entry>
  <entry>
    <title>statistic</title>
    <url>/2022/08/19/statistic/</url>
    <content><![CDATA[<h2 id="统计学知识梳理"><a href="#统计学知识梳理" class="headerlink" title="统计学知识梳理"></a>统计学知识梳理</h2><h3 id="多元正态分布"><a href="#多元正态分布" class="headerlink" title="多元正态分布"></a>多元正态分布</h3><p>对于一元正态分布而言，有如下密度函数</p>
<script type="math/tex; mode=display">
\begin{align}
f(x) &= \frac{1}{\sqrt{2\pi\sigma^2}}e^{-\left[(x-\mu)/\sigma\right]^2 / 2} \\
&= (2\pi)^{-1/2}(\sigma^2)^{-1/2}e^{-(x-\mu)'(\sigma^2)^{-1}(x-\mu) / 2}
\end{align}</script><p>其中的$(x-\mu)’(\sigma^2)^{-1}(x-\mu)$刻画了点$x$到均值$\mu$经过方差加权的距离，同理可以得到多元正态分布的密度函数（令维度为p）</p>
<script type="math/tex; mode=display">
f(\pmb{x}) = (2\pi)^{-p/2}|\pmb{\Sigma}|^{-1/2}e^{-(\pmb{x-\mu})'\pmb{\Sigma}^{-1}(\pmb{x-\mu})/2}</script><p>其中$\pmb{\mu}$为均值向量，$\pmb{\Sigma}$为协方差矩阵（对称正定）</p>
<p>e.g. 二元正态分布的密度函数</p>
<script type="math/tex; mode=display">
f(x_1, x_2) = \frac{1}{2\pi\sqrt{\sigma_{11}\sigma_{22}(1-\rho^2)}}\exp\left\{-\frac{1}{2(1-\rho^2)}\left[\left(\frac{x_1 - \mu_1}{\sqrt{\sigma_{11}}}\right)^2 + \left(\frac{x_2 - \mu_2}{\sqrt{\sigma_{22}}}\right)^2 + 2\rho\left(\frac{x_1 - \mu_1}{\sqrt{\sigma_{11}}}\right)\left(\frac{x_2 - \mu_2}{\sqrt{\sigma_{22}}}\right)\right]\right\}</script><span id="more"></span>
<p>从多元正态分布密度函数的表达式可以看出，对于一个固定大小c的密度而言，所有可能的$\pmb{x}$构成一个椭圆。</p>
<p><strong>引理1</strong></p>
<blockquote>
<p>Contours of constant density for the p-dimensional normal distribution are ellipsoids defined by $\pmb{x}$ such the that</p>
<script type="math/tex; mode=display">
(\pmb{x-\mu})'\pmb{\Sigma}^{-1}(\pmb{x-\mu}) = c^2</script><p>These ellipsoids are centered at $\pmb{\mu}$ and have axes $\pm c\sqrt{\lambda_i}\pmb{e_i}$, where $\pmb{\Sigma}\pmb{e_i}=\lambda_i\pmb{e_i}$ for i = 1, 2, …, p</p>
</blockquote>
<h4 id="一些常用的性质"><a href="#一些常用的性质" class="headerlink" title="一些常用的性质"></a>一些常用的性质</h4><ul>
<li>性质1: $\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$，则任意对于$\pmb{X}$中的随机变量的线性组合$\pmb{a’X} = a_1X_1 + a_2X_2 + \cdots + a_pX_p$服从分布$N(\pmb{a’\mu}, \pmb{a’\Sigma a})$；如果对于任意的$\pmb{a}$，都有$\pmb{a’X}$服从$N(\pmb{a’\mu}, \pmb{a’\Sigma a})$，则$\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$</li>
<li>性质2: $\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$，则$\pmb{AX} \sim N_q(\pmb{A\mu, A\Sigma A’})$，其中q是$\pmb{A}$的行数；$\pmb{X + d} \sim N_p(\pmb{\mu + d}, \pmb{\Sigma)}$，其中$\pmb{d}$为常数向量</li>
<li>性质3:$\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$，$\pmb{X}$的所有子集都是多元正态分布</li>
<li>性质4: <ol>
<li>$\pmb{X<em>1} \sim N_q(\pmb{\mu_1, \Sigma_1}), \pmb{X_2} \sim N_p(\pmb{\mu_2, \Sigma_2})$，则$Cov(\pmb{X_1, X_2}) = \pmb{0}</em>{q\times p}$</li>
<li>$\begin{bmatrix}\pmb{X<em>1} \ \pmb{X_2}\end{bmatrix} \sim N</em>{q<em>1 + q_2}\left(\begin{bmatrix}\pmb{\mu_1} \ \pmb{\mu_2}\end{bmatrix},\begin{bmatrix}\pmb{\Sigma</em>{11}} &amp;  \pmb{\Sigma<em>{12}}\ \pmb{\Sigma</em>{21}} &amp; \pmb{\Sigma<em>{22}}\end{bmatrix}\right)$，则$\pmb{X_1}, \pmb{X_2}$互相独立当且仅当$\pmb{\Sigma</em>{12}} = \pmb{0}$</li>
<li>$\pmb{X<em>1} \sim N_q(\pmb{\mu_1, \Sigma</em>{11}}), \pmb{X<em>2} \sim N_p(\pmb{\mu_2, \Sigma</em>{22}})$，且互相独立，则有$\begin{bmatrix}\pmb{X<em>1} \ \pmb{X_2}\end{bmatrix} \sim N</em>{q<em>1 + q_2}\left(\begin{bmatrix}\pmb{\mu_1} \ \pmb{\mu_2}\end{bmatrix},\begin{bmatrix}\pmb{\Sigma</em>{11}} &amp;  \pmb{0}\ \pmb{0} &amp; \pmb{\Sigma_{22}}\end{bmatrix}\right)$</li>
</ol>
</li>
<li>性质5: $\begin{bmatrix}\pmb{X<em>1} \ \pmb{X_2}\end{bmatrix} \sim N</em>{q<em>1 + q_2}\left(\begin{bmatrix}\pmb{\mu_1} \ \pmb{\mu_2}\end{bmatrix},\begin{bmatrix}\pmb{\Sigma</em>{11}} &amp;  \pmb{\Sigma<em>{12}}\ \pmb{\Sigma</em>{21}} &amp; \pmb{\Sigma<em>{22}}\end{bmatrix}\right)，给定$$\pmb{X_2 = x_2}$，则$\pmb{X_1}$的条件分布仍然为正态分布，且有$\pmb{\mu</em>{X<em>1 | X_2}} = \pmb{\mu_1 + \Sigma</em>{12}\Sigma<em>{22}^{-1}(x_2 - \mu_2)}, \pmb{\Sigma</em>{X<em>1|X_2} = \Sigma</em>{11} - \Sigma<em>{12}\Sigma</em>{22}^{-1}\Sigma_{21}}$</li>
<li>性质6: $\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$<ol>
<li>$(\pmb{X-\mu})’\pmb{\Sigma}^{-1}(\pmb{X-\mu}) \sim \chi_p^2)$，其中$\chi_p^2$是自由度为p的卡方分布</li>
<li>$\pmb{X} \sim N_p(\pmb{\mu, \Sigma})$中$ \left{(\pmb{x:} \pmb{x-\mu})’\pmb{\Sigma}^{-1}(\pmb{x-\mu}) \leq \chi_q^2(\alpha)\right}$的概率为$1-\alpha$，其中$\chi_q^2(\alpha)$是卡方分布的$\alpha$上分位点</li>
</ol>
</li>
<li>性质7: $\pmb{X<em>1}, \pmb{X_2}, …, \pmb{X_n}$互相独立且$\pmb{X_j}$服从分布$N_p(\pmb{\mu_j, \Sigma})$，则$\pmb{V_1} = c_1\pmb{X_1} + c_2\pmb{X_2} + \cdots + c_n\pmb{X_n} \sim N_p\left(\sum</em>{j=1}^n c<em>j\pmb{\mu_j}, \left(\sum</em>{j=1}^n c_j^2\right)\pmb{\Sigma}\right)$。令$\pmb{V_2} = b_1\pmb{X_1} + b_2\pmb{X_2} + \cdots + b_n\pmb{X_n}$，则$\pmb{V_1, V_2}$服从多元正态分布，协方差矩阵为<script type="math/tex; mode=display">
\begin{bmatrix}
\left(\sum_{j=1}^n c_j^2\right)\pmb{\Sigma} &\pmb{(b'c)\Sigma} \\
\pmb{(b'c)\Sigma} & \left(\sum_{j=1}^n b_j^2\right)\pmb{\Sigma}
\end{bmatrix}</script></li>
</ul>
]]></content>
  </entry>
  <entry>
    <title>tensorflow</title>
    <url>/2022/08/03/tensorflow/</url>
    <content><![CDATA[<h2 id="TensorFlow指北"><a href="#TensorFlow指北" class="headerlink" title="TensorFlow指北"></a>TensorFlow指北</h2><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><p>使用tensorflow_datasets加载数据的话需要挂vpn</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export HTTPS_PROXY=http://127.0.0.1:7890</span><br></pre></td></tr></table></figure>
<p>下载好的数据集会放在<code>~/tensorflow_datasets/downloads</code>下面，通常会有一个<code>tar.gz</code>和一个<code>INFO</code>文件，解压之后的文件会放在<code>extracted</code>里面，里面会有一个将<code>tar.gz</code>解压后的文件夹，会有一个<code>TAR_GZ.</code>的前缀。</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">tar.gz文件</span></span><br><span class="line">cs.toronto.edu_kriz_cifar-10-binaryxKOMUKG8XzocVTfyFVq51o-fJese2Nnd2j2ymlm8od0.tar.gz</span><br><span class="line"><span class="meta prompt_"># </span><span class="language-bash">解压后的文件</span></span><br><span class="line">TAR_GZ.cs.toronto.edu_kriz_cifar-10-binaryxKOMUKG8XzocVTfyFVq51o-fJese2Nnd2j2ymlm8od0.tar.gz</span><br></pre></td></tr></table></figure>
]]></content>
  </entry>
</search>
